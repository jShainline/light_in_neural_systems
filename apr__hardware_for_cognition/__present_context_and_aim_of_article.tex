\subsubsection{Present Context and Aim of this Article}
During WWII communication and information technology played many roles not present in prior conflicts. The technological and scientific climate was ideal for new advances in computing, particularly for general-purpose computers that could solve a variety of differential equations and thus contribute to many technical pursuits. Specifically, ... (list pursuits)

At the present moment, the technical areas driving evolution in machine learning and AI are myriad, far more diverse that during the 20th century. These driving influences include: consumerism, online advertising, and personal devices; financial security and financial trading; medical imaging, disease diagnosis, and drug development; data analysis and experimental control in scientific research; and information warfare. Also central to the present context are the unprecedented global transformation induced by silicon microelectronics, the associated availability of computational resources as well as large data sets, and the recent emergence of the Internet, which has led to the generation of large data sets and ensured that much of human activity involves engagement with an interconnected computational system.

This context is driving machine learning and AI to evolve in particular ways. Most importantly for the purposes of the present article, the aforementioned application spaces tend to require a computer that identifies patterns in a specific type of data. The system may seek patterns in a consumer's spend or internet browsing habits, or it may seek certain patterns in medical images or internet videos. The contemporary trend is thus to use machine learning to identify certain patterns in specific data types.

The objective of general intelligence requires more than category-specific pattern recognition or analysis of structured data. A system displaying general intelligence must be able to perceive and contextualize many types of data and learn to identify a large variety of patterns characterized by features in many dimensions of the data by observing and object's traits. For example, consider the associative intelligence involved in a mundane scenario encountered by a human. One may observe an object on the ground and based on its texture and shape know that the object is a leaf, and based on its location under a tree at its red and yellow coloring, it can be assumed the leaf has fallen from the tree. The intelligent system will recognize that this makes sense, as it is late autumn, and trees of this variety lose their leaves this time of year. The intelligent system may even conjure a mental image of a bud emerging in spring, demonstrating the recall of pertinent information to completely contextualize the elements under consideration. A model of the world exists within the intelligent system, and new information is continuously compared to this inner model. In turn, the inner model adapts to new information to maintain appropriate correspondence with the information to which it is exposed in a perpetually changing context.

Toward the goal of general intelligence, we must consider hardware that may depart from that which is adequate for domain-specific machine learning. Neuroscience is a valuable guide to the device and system requirements for hardware capable of general intelligence, while the history of computing and silicon electronic provide crucial insights related to the characteristics necessary in a successful, mature technology. This article reviews the neuroscientific principles that inform hardware properties. Several approaches to hardware for machine learning and neuromorphic computing are summarized, and attention is paid to selecting the physical mechanisms and devices that are best matched to the needs of neural computing for AGI. I describe the reasoning that leads me to the thesis that hardware utilizing few photons for communication and superconducting circuits for computation are uniquely suited to performing the functions required for intelligence. A potential architecture for such a system is outlined, extending from die-scale modules up to many-wafer cognitive systems with fiber-optic white matter connecting the system. Application spaces and potential physical limitations are discussed. This aim is to anticipate the hardware that will be utilized in mature, large-scale neural systems achieving general intelligence, and to explain the reasons why we should expect this hardware to differ from silicon microelectronics that have been unmatched in performance for digital computation. At this early stage, much remains uncertain about the feasibility of such a system, but the ramifications for technology and science are so immense that the subject deserves thorough inquiry.