\documentclass{article}

\usepackage{geometry}
\geometry{textwidth = 18cm,textheight = 24cm}

\usepackage{cite}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[dvipsnames]{xcolor}
\usepackage{multicol}
\usepackage{authblk}
  
\newcommand{\onlinecite}[1]{\hspace{-1 ex} \nocite{#1}\citenum{#1}} 

\newenvironment{Figure}
  {\par\medskip\noindent\minipage{\linewidth}}
  {\endminipage\par\medskip}

\title{Optoelectronic Intelligence}
\author[1]{\Large{Jeffrey M. Shainline\footnote{\textcolor{ForestGreen}{jeffrey.shainline@nist.gov}}}\\
\textit{\small National Institute of Standards and Technology, Boulder, CO, \textcolor{ForestGreen}{USA}, 80305}\\
\vspace{0.3em}
\small October 16th, 2020
}

\begin{document}

\maketitle

\begin{abstract}
\textcolor{ForestGreen}{General intelligence involves the integration of many sources of information into a coherent, adaptive model of the world.} To design and construct hardware for general intelligence, we must consider principles of both neuroscience and very-large-scale integration. For large neural systems capable of general intelligence, the attributes of photonics for communication and electronics for computation are complementary and interdependent. Using light for communication enables high fan-out as well as low-latency signaling across large systems with no traffic-dependent bottlenecks. For computation, the inherent nonlinearities, high speed, and low power consumption of Josephson circuits are conducive to complex neural functions. Operation at 4\,K enables the use of single-photon detectors and silicon light sources, two features that lead to efficiency and economical scalability. Here I sketch a concept for optoelectronic hardware, beginning with synaptic circuits, continuing through wafer-scale integration, and extending to systems interconnected with fiber-optic \textcolor{ForestGreen}{tracts}, potentially at the scale of the human brain and beyond.
\vspace{3em}
\end{abstract}

\begin{multicols}{2}

\section{\label{sec:introduction}Introduction}
General intelligence is the ability to assimilate knowledge across content categories and to use that information to form a coherent representation of the world. The brain accomplishes general intelligence through many specialized processors performing unique, complex computations \cite{ba1988,de2014}. The information generated by these processors is communicated throughout the network via dedicated connections spanning local, regional, and global scales \cite{sp2010}. On the micro-scale, synapses, dendrites, and neurons are specialized processors comprising the gray matter computational infrastructure of the brain \cite{geki2002}. On the meso-scale, cortical minicolumns of 100 neurons act as specialized processors \cite{mo1997}, and on the macro-scale, brain regions play that role \cite{brme2010}. Information is communicated between minicolumns and between brain regions via axonal fibers that comprise the white matter communication infrastructure of the brain. On short time scales, information processing occurs in synapses \cite{abre2004}, dendrites \cite{stsp2015}, and within single neurons \cite{ko1997}. On longer time scales, the information generated by minicolumns is communicated across wider regions of the network so that the knowledge of specialized processors can combine in a comprehensive interpretation of a subject \cite{bu2006}. The utilization of many specialized processors combining their shared computational resources across many scales of space and time enables the brain to achieve general intelligence \cite{ba1988,de2014}. 

Computation and communication are the complimentary pillars of neural systems. Hardware for artificial general intelligence (AGI) will achieve the highest performance if complex, local processors can pool the information from their specialized computations through global communication. Electrons excel at computation, while light is excellent for communication. In silicon hardware, monolithic optical links between a processor and memory have been demonstrated \cite{suwa2015}. These devices were fabricated in a 45-nm CMOS node with no in-line process changes, and off-chip light sources were utilized. Such work is driven by the desire for increased communication bandwidth in multi-core architectures. These architectures continue to expand into on-chip networks, in some cases resulting in highly distributed, brain-inspired systems implemented with CMOS electronics \cite{bo2000,pfgr2013,mear2014,fuga2014,payu2017,dasr2018}. As computing grows more distributed, communication becomes a bottleneck. A primary challenge affecting further chip-scale electronic-photonic integration is the difficulty of achieving a light source on silicon that is robust, efficient, and economical \cite{libo2010,zhyi2015}. Lessons learned from very-large-scale integration (VLSI) inform us that economical fabrication of integrated circuits comprising simple components is necessary for scaling. In this regard, difficulties associated with integrated light sources are the most significant impediment to optoelectronic VLSI.

It is the perspective of our group at NIST that hardware incorporating light for communication between electronic computational elements combined with an architecture of \textcolor{ForestGreen}{networked} optoelectronic spiking neurons will provide potential for AGI at the scale of the human brain. \textcolor{ForestGreen}{Here I use the term, ``spiking neurons'' to refer to circuits that integrate signals over time and produce pulses when an internal electrical threshold is reached. The spiking neurons discussed here are optoelectronic in that the pulses communicated from neurons to synapses consist of photons, while the computations performed within the neurons utilize electronic circuits. Each neuron contains a light source, which is driven by electronic circuits upon reaching threshold, and each synapse contains a detector, which converts the optical signal to an electrical current or voltage upon receiving a photonic synapse event. Each neuron is a separate entity, and no hardware components are multiplexed to represent the operations of separate neurons at different times.} While much of present-day computing infrastructure has evolved to implement a von Neumann architecture perfoming sequential operations in the model of a Turing machine, the functioning of neural systems departs considerably from this model. Light has even more to offer in a neural computing context, because communication across scales is indispensable. Further, the spiking behavior of Josephson junctions combined with the efficiency of single-photon detectors make a compelling case for optical integration with superconducting electronics \cite{shbu2017,sh2018}. Such a choice necessitates low-temperature operation near 4\,K. At this temperature, silicon light sources become available \cite{buch2017}, indicating that a major impediment to optoelectronic VLSI many not be present in the superconducting domain. This article summarizes the reasoning behind the assertion that superconducting optoelectronic systems have unique potential to achieve general intelligence when considered from the perspectives of cognitive science and VLSI.

\textcolor{ForestGreen}{In addition, the unique cognitive capabilities of human beings appear to derive in part from the sheer scale of the brain, where scale refers to the number of neurons, their degree of connectivity, and the complexity of the communication network that allows the information across the modular network to be integrated. While there is much to be gained from AI hardware at smaller scales, this article considers technological pathways to large cognitive systems, with tens to hundreds of billions of neurons, comparable to the human brain, and communication infrastructure of commensurate complexity. It appears that any such technology will require many interconnected wafers packed densely with integrated circuits. We may refer to this field of research as ``neuromorphic supercomputing'', which represents a grand scientific challenge. This effort is in some ways more akin to the construction of a fusion reactor than a microchip, and potentially offering a similar scale of societal benefit in the form of an experimental test bed enabling the elucidation of the mechanisms of cognition and the exploration of the physical limits of intelligence.}

\section{\label{sec:neuroscience}\textcolor{ForestGreen}{Neuroscience as a guide}}

\begin{figure*}[t!]
    \centering{\includegraphics[width=17.6cm]{complexity_across_scales.png}}
	\captionof{figure}{\label{fig:complexity_across_scales}Structure across scales. Biological systems have functional components from the nanometer scale (synapses, neurotransmitters and receptors, axonal pores, etc.) up to the full brain (roughly 0.3\,m linear dimension for full human cerebral cortex \cite{sh2019}). The speeds of various operations are limited by chemical diffusion and signal propagation along axons, which may ultimately limit the size of biological neural systems \cite{bu2006,sh2019}. The time constants associated with chemical diffusion and membrane charging/discharging span the range from 1\,ms to 100\,ms \cite{geki2002} and significantly influence the speeds at which information processing occur. The wall at the right indicates the communication-limited spatial-scaling barrier. By contrast, optoelectronic devices rarely have functional components with critical dimension smaller than 100\,nm, and optoelectronic neurons are likely to be on the 100-\textmu m scale, with dendritic arbor extending for millimeters and axonal arbor in some cases spanning the system. The time constants of these components can be engineered in hardware across a very broad range with high accuracy through circuit parameters, enabling rapid processing as well as long-term signal storage.  Yet optical communication between wafers enables vertical stacking into columns, and the high speed of devices and communication enables optoelectronic systems to extend far beyond the limits imposed by the slow conduction velocity of axons. In the large system illustrated at the right, photonic communication would enable every neuron of the system to simultaneously fire at its maximal rate with no traffic-induced communication delays.}
\end{figure*}

To guide the design of hardware for AGI, we must consider operation across spatial and temporal scales \cite{beba2017,khma2018}. \textcolor{ForestGreen}{Neuroscience provides key insights as to how neural systems integrate information across space and time to accomplish cognition \cite{bu2006,baga2011}. Here I provide a brief summary, highlighting the aspects most pertinent to the design of hardware for cognition.} 

\textcolor{ForestGreen}{In the temporal domain, consider the role of oscillations and synchronization \cite{bu2006} in structuring the activity of populations of neurons. The spiking activity of neurons is observed to comprise nested oscillations across the frequency spectrum \cite{budr2004,bu2006}. On the fastest time scales of the brain, local clusters of neurons engage in transient dynamical activity induced by the present stimulus. These patterns of activity are referred to as gamma oscillations (80\,Hz), and activity in this band is modulated by lower frequency oscillations \cite{caed2006,jeco2007} resulting from the combined activity of neurons across larger regions of the network \cite{stsa2000}. These slower, broader patterns are referred to as theta oscillations (6\,Hz), and neuronal communication across a network appears to depend in several ways upon gamma activity as messages being structured into more complex syntax by activity on theta timescales \cite{fr2015,bu2019}.}

\textcolor{ForestGreen}{In the spatial domain a feature of neural systems that will recur in the present discussion is their modular, hierarchical construction \cite{mela2010,beba2017,khma2018}. Neural systems are modular in that they are comprised of local regions of densely interconnected structures with sparser connectivity between such regions. Neural systems are hierarchical in that this pattern repeats across spatial scales in a fractal manner: minicolumns aggregate into columns, columns into complexes, etc. This fractal property is necessary to enable networks to scale arbitrarily, with dynamics constrained only by the physical hardware and spatial extent of the system rather than by the ability to communicate across the network \cite{plth2006}. Communication between distant modules, up the information processing hierarchy, is enabled by power-law scaling: the number of connections being sent to distant modules does not decay exponentially, but rather follows a power law \cite{bagr2010,spte2016}. The non-vanishing tail of long-range connections enables the firing activity of distant modules to quickly become correlated. In constructing hardware for artificial intelligence, it is imperative to enable rapid communication without traffic dependent bottlenecks. Modules must be able to rapidly engage in gamma activity, while signals from many interconnected modules at multiple levels of hierarchy must be able to simultaneously transmit across the complex network. The specific time scales defining behavior analogous to gamma and theta oscillations will be determined by the underlying computational devices.}

Figure \ref{fig:complexity_across_scales} charts the structures present on various scales for biological and optoelectronic hardware. The human brain has features spanning roughly eight orders of magnitude in size, from a nanometer to a tenth of a meter. Across time, activity ranges from the 1\,ms time scale of neurotransmitter diffusion across a synapse, through the 200\,ms time scale of brain-wide theta oscillations, up to the memory retention time of the organism. The speeds of devices and communication in the brain are limited by the chemical and ionic nature of various operations. The maximum size of the brain may be limited by the slow conduction velocity of ionic signals along axons. If the brain were larger, signals would not have time to propagate between different regions during the period of theta oscillations, and system-wide information integration could not be efficiently achieved \cite{bu2006,sh2019}.

\textcolor{ForestGreen}{In the form of gamma activity, clusters of neurons represent specific content,} and the information from these clusters must be shared with other regions of the network to form a multifaceted representation of a stimulus. This computation and communication is facilitated by networks with a high clustering coefficient yet also an average path length nearly as short as a random graph \cite{eskn2015}. \textcolor{ForestGreen}{In the language of network theory, if node $a$ is connected to node $b$, and $b$ is connected to $c$, then the clustering quantifies the probability that $a$ will be connected to $c$. Path length quantifies the number of intermediate nodes that must be traversed to get from one node to another along the network connections. The average path length is determined by calculating this quantity over all pairs of nodes in the network, and then taking the mean. A network with high clustering and low average path length is referred to as a ``small-world network'' \cite{wast1998}.} Such small-world networks are ubiquitous throughout the brain \cite{sp2010} and require long-range connections. In a random network, near and distant connections are equally probable, so the average path length across the network achieves a lower limit on path length for a given number of edges connecting a given number of nodes. Figure \ref{fig:path_length} shows the number of edges required per node to achieve a given average path length as a function of the number of nodes in the random network. For a modest network with one million nodes, each node must make one thousand connections to maintain a path length of two. For the case of a network with 100 million nodes, each node must make 10,000 connections. This is similar to the hippocampus in the human brain, with nearly 100 million neurons, each with 10,000 or more nearly random synaptic connections \cite{bu2006}. Maintaining a short path length across the network is critical for information integration, and is an important motivator to use light for communication.

Dynamical behaviors thought to be necessary for attention, cognition, and learning, such as cross-frequency coupling \cite{bu2006} and synaptic plasticity \cite{mage2012,ab2008,fudr2005}, require complex capabilities at the device level. Dynamical synapses, dendrites, and neurons allow one structural network to realize myriad functional networks adapting on multiple time scales. While light is excellent for communication, electrical circuits are better equipped to perform these nonlinear, dynamical functions. In particular, Josephson circuits naturally manifest many of the neuromorphic operations we seek \cite{sh2018,sh2020}. For communication and computation, neural information processing will benefit immensely from optoelectronic integration.

Light and electronics together can enable communication and computation across spatial and temporal scales. We have proposed a specific approach we see as most conducive to large-scale implementation for AGI \cite{shbu2017,sh2018,sh2019,sh2020}. The approach combines waveguide-integrated light sources and single-photon detectors for communication \cite{shbu2017,buch2017} with Josephson circuits for synaptic, dendritic, and neuronal computation \cite{sh2018,sh2020}. As illustrated in Fig.\,\ref{fig:complexity_across_scales}, these optoelectronic networks will have features as small as 100\,nm and potentially extend up to kilometers. Neuronal inter-spike intervals can be as short as 50\,ns, while synaptic and dendritic processing occurs on the 50\,ps time scale of Josephson junctions. Time constants can be chosen across many orders of magnitude, enabling information processing and memory across time scales. Figure\,\ref{fig:complexity_across_scales} is intended to emphasize that if communication barriers can be removed, neural systems of extraordinary scale can be achieved.

\begin{Figure}
	\centering{\includegraphics[width=8.6cm]{path_length.png}}
	\captionof{figure}{\label{fig:path_length}The average number of connections per node ($\bar{k}$) required to maintain a given average path length ($\bar{L}$)  across a random network as a function of the total number of nodes in the system ($N_{\mathrm{tot}}$).}
\end{Figure}

\section{\label{sec:synapses_dendrites_neurons}\textcolor{ForestGreen}{Superconducting o}ptoelectronic synapses, dendrites, and neurons}
\begin{Figure}
	\centering{\includegraphics[width=8.6cm]{spd_jj_synapse.png}}
	\captionof{figure}{\label{fig:synapse}\textcolor{ForestGreen}{Circuit diagrams. (a) Superconducting optoelectronic synapse combining a single-photon detector (SPD) with a Josephson junction and a flux-storage loop, referred to as the synaptic integration (SI) loop. (b) Neuron cell body performing summation of the signals from many synapses as well as thresholding. Here the neuronal receiving (NR) loop is shown collecting inputs from two SI loops, but scaling to thousands of input connections appears possible.}}
\end{Figure}
Having chosen to communicate synaptic events with light, the quantum limit is a single photon per synaptic connection. We have designed a synapse \cite{sh2018,sh2020} \textcolor{ForestGreen}{[Fig.\,\ref{fig:synapse}(a)]} that detects a single near-infrared photon and requires no power to retain the synaptic state, a feature enabled by the dissipationless nature of superconductors. The synapse utilizes a superconducting-nanowire single-photon detector (SPD), which is simply a current-biased strip of superconducting wire \cite{mave2013}. To achieve the desired synaptic operation, an SPD is combined in circuits with Josephson junctions (JJs) and superconducting loops to achieve the functions needed for neural information processing. In optoelectronic synapses of this design, the current bias across a single JJ establishes the synaptic weight \textcolor{ForestGreen}{[$I_{\mathrm{sy}}$ in Fig.\,\ref{fig:synapse}(a)]}. This current bias can be dynamically modified through various photonic and electronic means based on control signals or network activity. \textcolor{ForestGreen}{The signals from many synapses can be combined through transfers coupled to dendrites or neurons [Fig.\,\ref{fig:synapse}(b)]. Neurons constructed in this manner are highly modular in that synapses, dendrites, and the neuron cell body itself are all based on the same core circuit, comprising a superconducting quantum interference device (SQUID) embedded in a flux-storage loop. SQUIDs are perhaps the most ubiquitous of all superconducting circuits \cite{vatu1998,ka1999}, often used as sensors due to their extraordinary sensitivity to magnetic flux and low-noise operation. These properties make them ideal circuit components for use as dendrites and neurons that can perceive and respond to minute changes in analog signal levels.} Dendritic and neuronal nonlinearities are a natural consequence of the JJ critical current, \textcolor{ForestGreen}{and can be shaped through the choice of circuit parameters such as loop inductances, resistances, and dynamically with adaptive bias currents to enable broad diversity both within a given complex neuron and across large, multi-functional networks. These functions are described in more detail in previous publications \cite{sh2018,sh2019,sh2020}.} Due to the prominent role of superconducting current storage loops, we refer to these as loop neurons. We refer to networks of loop neurons as superconducting optoelectronic networks (SOENs). In the operation of loop neurons, a single photon triggers a synaptic event, and spike-timing-dependent plasticity is induced by two photons\textemdash one from each neuron associated with the synapse.

In addition to the choice of SPDs as the detectors in the system, we must also select a light source, which must be fabricated across wafers by the millions. Because our choice of detectors dictates cryogenic operation, silicon light sources are an option. The light sources we have in mind are silicon LEDs \cite{buch2017}, employing luminescence from defect-based dipole emitters. From the perspective of VLSI, achievement of a silicon light source as simple as a transistor would be the greatest contribution to the success of this technology. If cryogenic operation enables both single-photon detectors and silicon light sources, it will be worth the added infrastructure for cooling.

\begin{figure*}[tb]
    \centering{\includegraphics[width=17.2cm]{experimental.png}}
	\captionof{figure}{\label{fig:experiment}Experimental progress toward superconducting optoelectronic networks. (a) Schematic of waveguide-integrated silicon LED. (b) Microscope image of a silicon LED waveguide-coupled to a superconducting-nanowire detector. (c) Experimental data showing that light is coupled through the waveguide, while cross talk to an adjacent detector on the chip is suppressed by 40\,dB. (a-c) Adapted from Ref.\,\onlinecite{buch2017}. (d) Schematic of the superconducting thin-film amplifier. (e,f) The resistive switch driving the LED. (e) Square pulses are driven into the switch gate. (f) When the switch is driven, light is produced from the LED and detected by the SPD. (d-f) Adapted from Ref.\,\onlinecite{mcve2019}. (g) Schematic of multi-planar integrated waveguides for dense routing. (h) Schematic of feed-forward network implemented with two planes of waveguides. (i) Data from an experimental demonstration of routing between nodes of a two-layer feed-forward network with all-to-all connectivity. (g-i) Adapted from Refs.\,\onlinecite{chbu2017} and \onlinecite{chbu2018}.}
\end{figure*}
To achieve complex neural circuits, we aim for monolithic integration of light sources, detectors, and superconducting circuit elements. Our group's experimental progress towards this end is summarized in Fig.\,\ref{fig:experiment}. \textcolor{ForestGreen}{An important milestone was the demonstration of an all-silicon monolithic optical link.} We measured waveguide coupling of light from micron-scale, all-silicon LEDs to integrated, silicon-based SPDs on a silicon photonic chip (Fig.\,\ref{fig:experiment}(a-c), \cite{buch2017}). \textcolor{ForestGreen}{Further progress on scalability and characterization of waveguide-integrated SPDs for use in the synapses under consideration was also presented in Ref. \onlinecite{buta2020_2}.} The performance achieved in the first iteration of these optical links was not yet adequate. The observed efficiency was $5\times10^{-7}$, while $10^{-3}$ is desirable for large systems \cite{sh2019}. Yet the simplicity of both the source and detector made the fabrication and demonstration of a monolithic optical link far easier than if room-temperature operation were required, \textcolor{ForestGreen}{and further work has discovered techniques for improving the brightness of the sources by two orders of magnitude through optimized fabrication procedures \cite{buta2020}. Additional improvements are likely to result from optimization of the semiconductor diode structure used for electrical injection of carriers into the waveguide where electron-hole pairs recombine to produce waveguide-coupled luminescence. Careful treatment of surfaces through proper passivation and elimination of etching in the electrically active region may significantly reduce non-radiative recombination pathways. Improvements to the optical structure may increase coupling efficiency from the emitters to the waveguide mode.} For this application, the light sources are only required to produce incoherent pulses of 10,000 photons ($\approx 1$\,fJ) at 20\,MHz when operating at 4\,K. \textcolor{ForestGreen}{Only modest advances are required in silicon light source technology to meet these specificiation. However, few in the field of optoelectronics are working in this area, as these light sources offer no chance of meeting the needs of the well-established telecommunications industry.}

We have also demonstrated superconducting amplifiers capable of generating the voltage required to produce light from these sources (Fig.\,\ref{fig:experiment}(d-f), \cite{mcve2019}). Generating more than a millivolt with superconducting circuits is difficult, but the thin-film, micron-scale cryotron demonstrated in Ref.\,\cite{mcve2019} leverages the extreme nonlinearity of the superconducting phase transition to rapidly generate high impedance and voltage with low energy, thus driving a semiconductor light source during each neuronal firing event. In Ref.\,\onlinecite{mcve2019} we demonstrate the use of these amplifiers to drive the LED-SPD link of Ref.\,\onlinecite{buch2017}. \textcolor{ForestGreen}{Fabrication of these devices appears compatible with silicon microelectronic manufacturing, provided the high-temperature steps required for dopant activation contact annealing required for the semiconductor devices are performed prior to the deposition of the superconducting thin films.}

During neural operation, the light produced by these LEDs fans out across a network of micron-scale dielectric waveguides terminating on the superconducting detectors at each synaptic connection. We have demonstrated multiple vertically integrated planes of these waveguides (Fig.\,\ref{fig:experiment}(d-f), \cite{chbu2017}), and used them to implement the architecture of a feed-forward neural network with two layers of 10 neurons per layer and all-to-all connectivity \cite{chbu2018}.

\section{\label{sec:other_approaches}\textcolor{ForestGreen}{The landscape of research in photonic and superconducting neural systems}}
This approach to neural computing resides at the confluence of superconducting electronics and integrated photonics, and therefore must be contrasted with other work in these fields. \textcolor{ForestGreen}{At the outset, it is clarifying to acknowledge that nearly all the efforts to use photonics or superconducting electronics for neural systems are focused on the entirely reasonable goal of doing useful computations with hardware that is available right now. These efforts are valuable and promising for their own ends without seeking brain-scale cognition. By contrast, the superconducting optoelectronic hardware for which I advocate here is in an early stage of development, as described in Sec.\,\ref{sec:synapses_dendrites_neurons}. My comments here contrasting the proposed SOEN hardware with other concurrent efforts should not be received as criticism of any the work in the field, but rather as an explanation of the reasoning that becomes necessary if very large systems are the objective. Here I provide a short summary of other photonic and electronic efforts to place superconducting optoelectronic systems in context. For comprehensive reviews of efforts in emerging neural hardware, the reader is directed to recent reviews \cite{bexi2020,shta2021}.}

\subsection{Semiconductor electronic neural systems}
\textcolor{ForestGreen}{Given the extraordinary success of CMOS electronics, utilization of that hardware platform is the clear place to begin a search for artificial neural circuits. The history of exactly this pursuit is rich \cite{me1989,me1990,lide2015} and accomplished \cite{}, and it is certainly the case that exciting advances lie ahead \cite{}. So why advocate for change?} To further explain why we place optical communication at the center of hardware development, I briefly summarize the physical limitations of electrical interconnection networks \cite{hepa2012}. It is impracticable in silicon electronics for a single device to source current to many other devices. A shared communication network must be employed. Switched media networks are used for this purpose. Each device must then only communicate to the nearest switch in the network. Because the communication infrastructure is shared, devices must request and wait for access to the switch network to transmit messages. This approach to communication leverages the speed of electronic circuits to compensate for the challenge of direct communication. Limitations are reached when many devices must communicate with many other devices simultaneously. While neural activity is generally sparse, during \textcolor{ForestGreen}{peak activity, such as coordinated gamma bursting at the peak of a theta oscillation (Sec.\,\ref{sec:neuroscience})}, many neurons must communicate simultaneously across the network. \textcolor{ForestGreen}{Due to the traffic-dependent bottleneck of shared interconnection infrastructure, a}s more neurons are added to the network, the average frequency of neuronal firing events must decrease\textcolor{ForestGreen}{, and the nested frequencies of oscillations must shift lower due to delays}. Integration of information across the network is limited by the communication infrastructure.

\subsection{Optical neural systems}
\textcolor{ForestGreen}{One means to alleviate communication limitations is through the use of optics. The field of photonic neural systems began in 1985 \cite{psfa1985,faps1985} with an implementation of the Hopfield model that had been introduced three years earlier \cite{ho1982}. The objective was to combine the parallelism and interconnectability of optics, which are linear phenomena, with bistable optical devices to provide the thresholding nonlinearity of the Hopfield model. The hardware proposed in 1985 combined compound-semiconductor LEDs with photodiodes and electronics for a initial implementation of nonlinearity to be replaced by optical bistable devices in subsequent generations. While LEDs and laser diodes have become mature technologies, bistable optical devices have not.}

\textcolor{ForestGreen}{The field of photonic neural systems has since experienced an immense diversification, with myriad efforts using free-space optics, fiber components, on-chip integrated waveguides, passive devices, and detectors. Along one branch of this tree}, excitable lasers have been explored as spiking neurons \cite{prsh2017}. These lasers integrate several optical inputs, and release a laser pulse upon reaching threshold. These devices can be extremely fast, but consume too much power for scaling to the level of the human brain. Excitable lasers can be used as spiking neurons in the broadcast-and-weight architecture \cite{tana20142}, wherein each neuron is assigned a wavelength, and synaptic weights are established with microring resonators that attenuate the optical signals, much like waveglength-division-multiplexed fiber-optic networks. In conventional silicon photonics \textcolor{ForestGreen}{\cite{li2005}}, such multiplexing employs around 10 channels. It may be possible to extend this to 100 \cite{prsh2011,tana20142}, but \textcolor{ForestGreen}{this limited number of channels} would require cumbersome control circuits to hold synaptic weights stable. The requirement of precise control at every synapse as well as the non-monotonic, rapidly varying Lorentizian lineshape of microring resonances is not optimal for large-scale, unsupervised learning. 

Phase change materials have also been explored for neuronal thresholding \cite{chsa2018} and as a means of implementing variable attenuation of photonic signals to establish a synaptic weight \cite{chri2017}. This approach requires billions of photons to achieve synaptic weight modification. Relying on the properties of a material to achieve the complex computations occurring at a synapse allows limited functionality as compared to dehaviors that can be tailored with integrated circuits. Deep learning with continuous fields rather than spiking neurons is also receiving attention, and networks of on-chip, cascaded Mach-Zehnder interferometers are a prominent approach \cite{shha2016}\textcolor{ForestGreen}{\textemdash an integration instantiation of photonic matrix-vector multiplication originally implemented in free space \cite{godi1978}}. Such networks excel at feed-forward processing operations, but are not conducive to establishing the recurrent networks employed by spiking neural systems nor the activity-dependent plasticity necessary for unsupervised learning. \textcolor{ForestGreen}{The challenge arises because in such meshes, adjustment of a single phase modifies multiple synaptic weights. While such a technique may be suitable for specific training algorithms employed for supervised learning \cite{humi2018}, it appears cumbersome for unsupervised learning in large neural systems, where local activity at each synapse updates that synaptic weight.}

\textcolor{ForestGreen}{Another exciting and related application space of photonics is in reservoir computing. This field has been very innovative and productive in recent years, but the objectives and hardware are only loosely related to the subject of large-scale cognition considered here, so I direct the interested reader to the relevant literature \cite{funa1993,vada2011,orso2015,vabr2017,brpe2018}.} 

\textcolor{ForestGreen}{Most photonic neural systems encode information in the amplitude of optical signals received at a detector, and synaptic weights are established through modulation of the intensity of these optical signals. Whether phase modulation and coherent intereference or direct amplitude modulation are leveraged, encoding synaptic weights in the intensity of light on a detector differs from the synaptic operations we are pursuing, where light is used for binary communication, and synaptic weights are established entirely by electronic responses. This approach minimizes the optical power required. Using light pulses for binary signaling also eliminates a source of noise. If synaptic weights are encoded in the intensity of an optical signal, noise from the light source is convoluted with the synaptic weight. With binary optical signaling the light level incident upon a synaptic detector does not influence the electronic response of the synapse, which is determined by the electronic circuits reading out the synaptic receiver. A binary response to a light pulse can be achieved with semiconductor receivers or superconducting circuits. In Ref.\,\onlinecite{buta2020_2} we have shown that the response of a superconducting SPD is independent of the number of photons present in an incident pulse across four orders of magnitude of input intensity. Because no information is encoded in the light level, this form of optical communication does not suffer from typical shot noise. Provided one or more photons is received by the detector, a synapse event is communicated. One can use the Poisson distribution to calculate the probability that zero photons are received. If the average number of photons transmitted per synapse is five or greater, the probability of receiving zero photons is less than 1\%, a considerably lower error rate than biological synaptic transmission \cite{li1997}. We therefore assume each neuronal light source will generate 10 photons per synaptic connection to accommodate 3\,dB of propagation loss while achieving 99\% transmission success rate. All energy and power consumption estimates presented here assume this value of 10 photons per synaptic communication event.}

\subsection{Superconducting electronic neural systems}
Approaches to neural computing using superconducting circuits leverage the nonlinear properties of Josephson junctions \cite{hias2007,sele2017,scdo2018}. \textcolor{ForestGreen}{Like photonic neural systems, these efforts began in the 1980s \cite{ai1989,og1989}. The objective of early superconducting neural circuits was to perform the weighted summation and thresholding operations required in the computational primitives of ANNs \cite{hago1991,hiak1991}. The circuits employed were similar to those utilized in superconducting digital logic, as were the basic concepts, such as using an up-down counter to implement synaptic weights \cite{hiak1991}. From the beginning, and continuing to the present day \cite{sckl2016,klsc2018,sosc2018}, attention is paid to sculpting a sigmoidal transfer function to implement back-propagation as well as alternative circuits for achieving Hebbian-type learning \cite{hago1991}.} 

\textcolor{ForestGreen}{More recent efforts have broadened attention to consider also spiking neural systems, leveraging the inherent threshold and spike production of JJs \cite{crsc2010,scdo2018}.}  The most successful experimental effort to date demonstrated coupling of two neurons based on JJs, with inter-spike intervals on the order of tens of picoseconds \cite{sele2017}. \textcolor{ForestGreen}{Additional progress has been made in synaptic memory technology based on magnetic JJs, wherein magnetic nanoclusters embedded in the tunneling barrier of a JJ are re-oriented by current pulses, thereby providing a means to modify the junction critical current \cite{scdo2020} to dynamically reconfigure the response of a synaptic circuit. Such devices are highly analogous to memristors being pursued for use in semiconductor-based neural systems \cite{kiha2018}.} 

\textcolor{ForestGreen}{The superconducting circuits discussed in the present context have much in common with other contemporary efforts in JJ-based neural systems \cite{crsc2010}. One point of contrast is that our effort is on analog computations enabled by the use of large flux-storage loops capable of representing thousands of signal level, whereas other efforts are primarily focused on the high-speed and energy efficiency enabled by the use of single fluxons. Yet this distinction is minute in comparison to the significant difference in hardware introduced by the choice to employ photonic communication. From my perspective, the challenge with using superconducting electronics alone to enable large-scale cognitive system is communication. In superconducting circuits, direct fan-out is usually limited to two, so for neurons to make thousands of connections, many stages of pulse splitters and active transmission lines must be employed. This leads to a cumbersome communication network requiring many JJs and severe challenges for wiring and routing. Fan-out and fan-in has recently been analyzed in these systems, and it has been argued that there is no fundamental limit to fan-out \cite{scse2020}. Interestingly, the Ref.\,\cite{scse2020} identified fan-in as a limiting factor. However, the use of analog synaptic integration loops with high inductance eliminates this bottleneck \cite{sh2019}.}

\textcolor{ForestGreen}{It is correct that fan-out challenges with superconducting circuits are not fundamental, and reasonable researchers in the field can disagree about the scale at which practical limits will be reached. For long-distance communication, pulses produced by JJs must be regenerated along active transmission lines. These transmission lines use JJs spaced periodically to re-transmit pulses, and the spacing of these JJs is set by inductance requirements. Using typical superconducting wires, a pitch of 100\,\textmu m between these junctions is expected, meaning a neuron trying to reach a synapse on the other side of a 1\,cm\,$\times$\,1\,cm die will require 100 JJs for communication to that synapse. At the scale of a 300\,mm wafer, 10,000 JJs would be required for long-range connections. Each of these JJs must be provided with a current bias. While many synaptic connections are more local, the requirement for long-distance connections is paramount in neural systems, as described in Sec.\,\ref{sec:neuroscience}. To go further and envision many-wafer systems containing billions of neurons and extending over meters all connected with active, superconducting transmission lines does not appear promising to me. I do not argue that such a communication network is fundamentally impossible, but rather that if the hardware for passive photonic communication proves feasible, scaling to massively interconnected spiking neural systems will be greatly facilitated.}

\subsection{Optoelectronic neural systems}
\textcolor{ForestGreen}{Speaking generally, the vast majority of} photonic and superconducting electronic neuromorphic efforts are focused on attaining relatively small-scale systems to accomplish specific computational functions\textemdash programmable accelerators or experimental test beds rather than cognitive systems. The superconducting optoelectronic approach championed here utilizes similar superconducting circuits as Refs.\,\onlinecite{hias2007,sele2017,scdo2018} for synaptic, dendritic, and neuronal computation, while leveraging light for communication, thereby enabling scaling to massively interconnected systems. Electrons and photons are complimentary, and optoelectronic integration is most straightforward when combining superconducting circuits with silicon light sources operating at liquid helium temperature.

\textcolor{ForestGreen}{In addition to contrasting this approach to other existing work in the field, it is necessary to compare to what may seem a more straightforward route to optoelectronic intelligence. This route would involve spiking neurons based on waveguide-integrated light sources, as we have discussed, but instead of SPDs and JJs, semiconductor photodiodes and transistors would be employed. Such hardware has not yet come to fruition, in part due to the challenges related to light-source integration. Nevertheless, the proposition that superconducting electronics are more promising than MOSFETs for this application requires justification.}

\textcolor{ForestGreen}{Our choice to focus on the superconducting approach is based primarily on three factors. First, superconducting single-photon detectors dramatically reduce the brightness required of the light sources. While semiconducting detectors, such as avalanche photodiodes, can detect a single photon, the energy consumption negates the benefits of single-photon sensitivity in the system application under consideration. For scalable system integration, the semiconductor counterpart to a waveguide-integrated SPD working in conjunction with a JJ is a waveguide-integrated photodiode working in conjunction with a MOSFET. Such a semiconductor receiver is likely to require roughly 1000 photons to charge the capacitance of the MOSFET gate \cite{mi2017} to initiate a synapse event. This factor of 1000 in photon power is matched by the factor of 1000 incurred to cool the superconducting system, so the net power consumption for light generation in semiconductor and superconductor systems is roughly equivalent. Yet the important distinction is that the superconducting system dissipates this power off chip in a cryocooler, whereas the semiconducting system requires the light sources to produce this power in the form of photons. Optoelectronic neural systems leveraging superconductors can make due with light sources capable of providing 10,000 photons within a few tens of nanoseconds (30\,nW continuous-wave equivalent), while a semiconducting counterpart will require light sources 1000 times brighter to attain the same firing rate. Achieving the former appears possible with inexpensive silicon light sources, while the latter is likely to require more costly III-V sources. While exciting progress continues to be made in III-V integration on silicon \cite{hala2020}, a central challenge remains to integrate these light sources intimately with electronics. The system under present consideration further requires fabrication by the millions across 300-mm wafers, which will surely be more cost effective, even in the long term, if silicon devices as simple as transistors can be employed for light emission \cite{buch2017}.}

\textcolor{ForestGreen}{The second factor driving our group to pursue the superconducting approach relates to multi-planar wafer-scale integration. Whether semiconductors or superconductors are employed, synaptic, dendritic, and neuronal circuits are not small. To accommodate millions of neurons and their synapses on a 300-mm wafer, on the order of 20 planes of photonic waveguides are required for communication, and a similar number of planes of electronic circuits are likely to be necessary. For each plane of MOSFETs, high-temperature annealing steps are required for dopant activation, leading to processing challenges when integrating with metal wires, photonic waveguides, and light sources. This processing challenge is one reason it has been difficult to extend MOSFET processes to multiple stacked planes of transistors with copper interconnects between. Power dissipation and heat removal also come into play but may be less consequential in the context of spiking neurons with sparse activity. By contrast, superconducting electronic circuits are processed near room temperature, and the prospect of integrating many planes of JJs, SPDs, and waveguides appears to us to be less restrictive. Multiple planes of active SPD \cite{vema2012} and JJs \cite{} have been demonstrated.}

\textcolor{ForestGreen}{The third factor steering us toward superconducting electronics relates to memory and learning. For a cognitive system of the scale under consideration, synaptic weight modification must be unsupervised and will be most readily realized if the signals that induce learning functions are the same signals, with the same current, voltage, or light levels, used for computing within neurons, and sent to synapses for communication. With superconducting circuits, single-flux quanta are used for computing, and single-photons are used for communication. It appears possible for these same signals to update synaptic weights and enable learning, primarily by adjusting current biases to JJs. A close functional analogy would be to modify the voltage on the gate of a MOSFET in an analog manner, and indeed, this has long been the ambition of floating-gate MOSFETs for synaptic memory \cite{hama2013}. However, the voltages required to change the charge on the gate are much higher than typical voltages used for computation elsewhere within the circuit, making it difficult to implement unsupervised learning based only on the signals already present in the network. These persistent challenges with floating gates have led to many efforts looking elsewhere for suitable adaptive circuits \cite{upji2019}. While any one of these efforts may lead to the desired memory operations, it is our perspective that the path to systems with lifelong and a multitude of memory mechanisms appear less formidable with Josephson circuits.}

\textcolor{ForestGreen}{Despite these arguments in favor of superconducting electronics, several valid counterpoints can be raised. The requisite silicon light sources and scale of cryogenics remain far from proven and may be prohibitively expensive. Massively multiplanar fabrication of superconducting optoelectronic wafers remains an ambitious technological undertaking, by no means guaranteed to succeed. Further research must be conducted to determine what is possible. For many readers, the requirement of cryogenic operation is the most disconcerting aspect of the project. Several comments are in order. Low-temperature operation eliminates such systems from consideration for applications that require low system power consumption, such as mobile devices. Even for larger applications, it may be possible that cryogenic operation proves to be an insurmountable obstacle. Yet the field of quantum information indicates otherwise. Many qubits require operation at a few tens of millikelvin, necessitating the extra expense and complexity of dilution refrigerators. The environment at 4\,K is balmy by comparison. Quantum information presently enjoys tremendous investment because these systems are thought to be capable of computations not otherwise possible. If it is to have a future, the same must be true of superconducting optoelectronic intelligence. Anything that can be done with CMOS will be done with CMOS. If SOENs cannot achieve AGI that is otherwise unattainable, they will not be brought into existence. If they can attain unmatched cognition, someone is likely to be willing to pay for them, unless the expense is astronomical. The perspective presented here is that exactly this will come to pass: superconducting optoelectronic hardware will enable AI that simply cannot be achieved through other physical means. Low-temperature operation will be justified by the performance.}

\textcolor{ForestGreen}{As a final perspective on the subject of low-temperature operation, consider that the vast majority of the universe is in thermal equilibrium with the cosmic microwave background at 2.7\,K, below the proposed operating temperature of SOENs. In such a setting, all system power consumption estimates are reduced by a factor of 1000 from the numbers presented here, at which point the energy consumption per synaptic operation of such systems rivals that of the human brain, while enabling firing rates at least 100,000 times faster. There is no reason to expect mature technological intelligence to share our disposition toward an environment where water is liquid, and may instead prefer to reside in an environment where helium condenses. From the point of view of an investor hoping to profit from such systems in the coming decades, this may bring little solace. But if our goal is to answer scientific questions regarding the physical limits of cognition, low-temperature operation may be of little long-term consequence.}

\section{\label{sec:communication}Scaling an optoelectronic system}
The physics of light is complementary to that of electrons. Photons can co-propagate on a waveguide independently without capacitance. Waveguides can fan out without a charging penalty due to wiring. This is not to say photonic communication can address an arbitrarily large number of recipients without consequence. For each new recipient, the number of photons in a neuronal pulse must increase. As destinations get further away, more energy is dissipated to propagation loss. These realities notwithstanding, it is feasible for devices communicating with photons to make direct, independent connections to thousands of destinations, thereby eliminating the need for the shared communication infrastructure that is the primary impediment to achieving AGI with electrical interconnections.

Having made this claim, the burden is upon us to provide evidence of the feasibility of photonic communication in large-scale neural systems. The large wavelength of light relative to the size of electronic devices causes concern for the size of optoelectronic brain-scale networks. To build confidence for the feasibility of the endeavor, I sketch here a vision of how such an optoelectronic neural system may be constructed. At the foundation of this vision is the assumption that the technology will utilize the fabrication infrastructure of silicon electronics and photonics in conjunction with fiber optics for longer-range communication. 

\begin{Figure}
	\centering{\includegraphics[width=8.6cm]{nodes_on_wafer.png}}
	\captionof{figure}{\label{fig:nodes_on_wafer}The total number of nodes that can fit on a 300\,mm ($N_{300}$) wafer as a function of the number of connections per node ($k$) for various numbers of waveguide planes in the wire-limited regime \cite{ke1982}.}
\end{Figure}
At the wafer scale, light will be guided in multiple planes of dielectric waveguides \cite{chbu2017,chbu2018} (Fig.\,\ref{fig:communicationAcrossScales}(a)), just as integrated electronics requires multiple wiring layers. To estimate the area of such photonic interconnection networks, we follow Keyes \cite{ke1982} and approximate the number of neurons that can be supported on a 300-mm wafer by $N = 2\sqrt{2}r^2\left(p/wk_{\mathrm{in}}\right)^2$. Here, $p$ is the number of planes of waveguides, $w$ is the waveguide pitch (1.5\,\textmu m), $k_{\mathrm{in}}$ is the number of waveguides entering the neuron, and $r = 150$\,mm. The prefactor results from assuming octagonal tiling. This expression is plotted in Fig.\,\ref{fig:nodes_on_wafer}. The estimate informs us that a 300-mm wafer with six waveguide planes can support roughly one million neurons if they each have one thousand connections. More involved analysis finds more planes may be needed \cite{sh2019}. As a point of comparison to electrical neural systems, Ref.\,\onlinecite{kuwa2017} finds that through multi-layer, wafer-scale integration of logic and memory, 250 million electrical neurons could fit on a 300\,mm wafer. The trade-off is speed, as the shared communication network would limit the electrical neurons studied in Ref.\,\onlinecite{kuwa2017} to 10\,Hz operation when 1000 synaptic connections are made per neuron. Nevertheless, the message of Fig.\,\ref{fig:nodes_on_wafer} is that photonic routing results in large area consumption. An optoelectronic brain larger than that of a bumble bee will not fit on a single 300-mm wafer.

\begin{figure*}[tb]
    \centering{\includegraphics[width=17.6cm]{spatial_hierarchy.png}}
	\captionof{figure}{\label{fig:communicationAcrossScales}Hierarchical construction of optoelectronic neural systems. (a) Schematic of the process stack, with silicon light sources on a silicon-on-insulator wafer, waveguides and detectors above, followed by the Josephson infrastructure and mutual inductors for dendritic processing. (b) Vertical photonic communication between two stacked wafers. Liquid helium flows between the wafers of a column for cooling, and free-space links propagate without loss through the helium. The inset shows a schematic of a single 300\,mm wafer, with neurons and routing, cut into an octagon for tiling. (c) Illustration of in-plane tiling. Lateral wafer-edge links connect wafers in a plane, and fiber optic bundles fill the voids between wafers for long-range communication. (d) A large neural system with multiple large modules, each containing hundred to thousands of wafers, enabled by photonic communication and the efficiency of superconducting detectors and electronics. Not shown is the fiber-optic white matter that would be woven through the voids between the octagons in this example hierarchical tiling.}
\end{figure*}
Optoelectronic intelligence will require communication between wafers. Wafers can be stacked vertically, and free-space optical links can send photons from a source on one wafer to a detector on a wafer above or below \cite{caga2012}, as illustrated in Fig.\,\ref{fig:communicationAcrossScales}(b). Assuming SPDs receiving vertical communication have a pitch of 25\,\textmu m, a 300-mm octagon could support $10^8$ vertical communication links between two wafers. Considering wafers as laminar layers, as in cortex, such a configuration would result in roughly 5\% inter-layer connectivity, similar to the fraction observed in mammals (Ref.\,\onlinecite{bu2006}, pg. 286).

In addition to feed-forward and feed-back free-space vertical coupling, lateral inter-wafer communication can be achieved at wafer edges, as shown in Fig.\,\ref{fig:communicationAcrossScales}(c). In the tiling considered here, each wafer makes such connections to neighbors in the cardinal directions. With a 10\,\textmu m pitch, 11,500 wafer-edge couplers could be supported in each direction. Such a system would demonstrate strong connectivity within the vertical stack of the wafers, and weaker lateral connectivity. The reader may recognize the columnar organization of the cerebral cortex \cite{mo1997}.

To achieve communication from within these columns to other regions of the network, optical fibers are ideal. Within the tiling under consideration, the square areas at diagonals between wafers can support fiber-optic bundles (Fig.\,\ref{fig:communicationAcrossScales}(c)). These optical fiber tracts are analogous to white matter in the brain. One such region could house a million single-mode fibers of 125\,\textmu m diameter. These fibers will emanate from all wafers within the column, and if six wafers are stacked in a column, each wafer would have 167,000 output fibers to carry information to other regions. With one million neurons on a wafer, not every neuron would have access to a fiber for long-distance communication (unless wavelength multiplexing is employed). This again is consistent with brain organization, wherein the number of long-distance axons emanating from a region is smaller than the number of neurons within the region. Each of these fibers can branch as it extends through the white matter, so a neuron with access to a single wafer-edge fiber can establish multiple long-range connections. \textcolor{ForestGreen}{Recent progress in low-loss fiber-to-waveguide coupling \cite{khbu2020} indicates a potential future direction for such integration of fibers with on-chip waveguides, but significant advances in manufacturing are required to realize the coupling of dense fiber bundles to 300-mm wafers.}

With this columnar configuration in mind, one can assess the feasibility of constructing a system on the scale of the human cerebral cortex (10 billion neurons, each with thousands of synaptic connections). If a wafer holds a million neurons, a cortex-scale assembly requires 10,000 wafers. Assuming the volume of white matter scales as the volume of grey matter to the 4/3 power \cite{zhse2000}, the cortex-scale system would fit in a volume two meters on a side. While optoelectronic neurons are significantly bigger than their biological counterparts, it is not the absolute size that limits system performance. The relevant quantity for assessing scaling limitations is the size of a neuron divided by the velocity of communication \cite{sh2019}. Communication at the highest velocity in the universe more than compensates. 

Regarding power, a single 300-mm wafer with a million neurons would dissipate one watt if the light production efficiency were $\eta = 10^{-4}$, a conservative estimate. For the cortex-scale system of 10,000 wafers, the device power consumption with $\eta = 10^{-4}$ would be 10\,kW. A further power penalty of one thousand would be incurred if the system were operated in a background of 300\,K. Thus, even in a conservative case of poor light production efficiency, an AGI on the scale of the human brain would consume 10\,MW, the same order as a modern supercomputer. We are considering a system with roughly the same number of neurons and synapses as the human cerebral cortex, but with activity at 30,000 times the speed. While there is high uncertainty associated with scaling estimates of such an immature technology, these calculations indicate that artificial brain-scale systems with photonic communication and electronic computation may be feasible, a possibility with profound implications for the future of science and technology. 

\section{\label{sec:discussion}Summary and Discussion}
I have argued that artificial neural hardware should be designed and constructed to leverage photonic communication while performing synaptic, dendritic, and neuronal functions with electronic devices. Superconducting optoelectronic circuits elegantly implement these functions, in part because of the utility of Josephson nonlinearities for neural computation, and also because superconducting detectors enable few-photon signals, approaching the lowest possible energy for optical communication. We have demonstrated all of the core components and are working toward complete integration.

This approach to AGI appears possible for physical and practical reasons. Physically, due to photonic signaling, it is possible to achieve efficient communication across the network for systems with orders of magnitude more than the 10,000 wafers comprising a brain-scale system. Reference\,\onlinecite{sh2019} explores the communication-limited size of the system as a function of the frequency of network oscillations. Specialized processors with activity at 20\,MHz (the gamma firing rate of loop neurons) can span an area 10 meters on a side before delays limit communication. Modules with activity at 1\,MHz (the frequency of corresponding theta oscillations in this system) could integrate information across an area the size of a data center within a single theta cycle.

On the practical side, fabrication of SOENs at industrial scale appears feasible. All the proposed circuits can be created on 300-mm wafers with existing infrastructure, such as a 45-nm CMOS node. Ten thousand wafers move through such a foundry every day. If dedicated to fabrication of optoelectronic intelligence, a foundry could produce multiple brain-scale systems per year. While the devices employed here depart from conventional silicon microelectronics, the same fabrication infrastructure can be employed. 

\textcolor{ForestGreen}{The approach to optoelectronic hardware described here is not without limits, and different factors limit performance at different scales. Regarding speed, the synaptic response is limited by the reset time of the SPD, which is between 10\,ns and 50\,ns depending on the material used. This response time limits the maximum useful neuronal firing frequency of the neuron to the range of 20\,MHz to 100\,MHz. Depending on the light source, the neuronal response during light production may have different limits. For the silicon light sources we have primarily been pursuing, the emitter lifetime is on the order of 40\,ns \cite{buta2020}, arriving at a maximum firing frequency comparable to the 20\,MHz figure determined by the speed of SPDs. In biological neural systems, conduction delays are an important factor limiting speed. Using light for communication greatly alleviates this concern, yet there does exist a system scale where the speed of light becomes the limiting factor. Within the 50\,ns reset time of the SPD or the comparable 40\,ns lifetime of the silicon emitters, light can travel over 10\,m. A system of this linear extent would contain at least an order of magnitude more neurons than an entire human brain. The scale set by this speed limit does not represent the maximum possible scale of an optoelectronic neural system, but rather the maximum possible volume of neurons that can communicate within the highest frequency oscillations of the system.}

\textcolor{ForestGreen}{Regarding power consumption, cryogenic cooling plays a key role. The power required for cooling  contains two contributions: the base-level power required to keep the environment below the superconducting transition temperature, even when the devices are inactive, and the additional cooling power required to remove excess heat generated by the activity of the circuits. The first factor is a few hundred watts for small systems, while the second factor is typically about one kilowatt of extra cooling power per watt of power dissipated by the devices. For small systems comprising a few thousand neurons each with a few hundred synapses on a 1\,cm\,$\times$\,1\,cm die, the devices will dissipate around a milliwatt \cite{sh2019}, so the first factor dwarfs the second. The second factor does match the first until intermediate-scale systems with tens to hundreds of interconnected wafers, each dissipating 1\,W-10\,W when active. It is somewhere between the scale of a few thousand neurons on a die and a few million neurons interconnected across several wafers that we expect the performance of the system to exceed what can be accomplished without photonic communication superconducting electronic computation. For large systems in excess of hundreds of interconnected wafers, the power dissipated by the active devices on the wafer and the associated cooling costs dominate. The power consumed by each wafer contains contributions from light sources, detectors at synapses, and JJs performing computations. If light sources can be realized with 1\,\% efficiency, each of these circuit components will contribute nearly equally to the total system power consumption \cite{sh2020}.}

What are the next steps to realize loop neurons and SOENs? Low-cost source-detector integration at the wafer scale is required. Active devices must be augmented with improvements in deposited dielectrics to enable many planes of routing waveguides with low loss. Hardware improvements will not lead to AGI without further theoretical insights. Conceptual advances are required to achieve high-performance neural systems, train them, and make them intelligent.

\vspace{2em}
This is a contribution of NIST, an agency of the US government, not subject to copyright.

\section{\label{sec:acknowledgements}Acknowledgements}
I acknowledge significant contributions to this project from team members Dr. Sonia Buckley, Dr. Jeff Chiles, Dr. Saeed Khan, Dr. Adam McCaughan, \textcolor{ForestGreen}{Bryce Primavera}, and Dr. Alexander Tait. This work would not be possible without the group leadership of Dr. Sae Woo Nam and Dr. Richard Mirin and the institutional support of NIST.

\vspace{2em}
Data available on request from the authors.

\bibliographystyle{unsrt}
\bibliography{optoelectronic_intelligence}

\end{multicols}

\end{document}