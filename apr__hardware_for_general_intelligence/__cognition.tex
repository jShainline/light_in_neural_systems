\subsection{\label{sec:cognition}Cognition}
Vernon Mountcastle's work on the columnar organization of the cortex was referenced in Sec.\,\ref{sec:modularity_and_hierarchy}. This work was presented In June of 1977 at the Neurosciences Research Program two-week meeting for intensive study of recent developments in neuroscience. Mountcastle presented his work in a keynote address in which he described his latest understanding of the physical structures comprising the cortex (cortical columns) and the manner in which they process and communicate information \cite{mo1978}. On the last afternoon of the workshope, Gerald Edelman put forth a theory of higher brain function based on processing performed by groups of neurons and a hierarchical model of feed-forward and feed-back communication \cite{ed1978}. The theory put forth by Edelman was consistent with and supported by the anatomical evidence presented by Mountcastle, and the resulting framework established a new foundation on which many subsequent ideas have been based. The lectures by Mountcastle and Edelman are now published in a single volume \cite{edmo1978} that represents the beginning of the modern phase of theories of cognition.

\subsubsection{Edelman's Model of Cognition}
In Edelman's model, neuronal groups containing 50 to 10,000 neurons serve to recognize a specific set of signals received either through sensory stimulus or from other groups of neurons. Edelman proposed that patterns in sensory input are first identified by a primary set of neuronal groups, and the set of all input patterns that can be recognized and induce a unique response constitute the primary repertoire. At the next level of information processing hierarchy, the secondary repertoire recognizes patterns in the responses of the groups in the primary hierarchy. Stimulus selects certain neuronal groups to become active, and over time, the adaptive nature of neurons and synapses makes certain groups much more likely to be excited by a specific stimulus. Edelman proposed that these neuronal groups are cyclically polled to determine which groups were most active and therefore which stimulus was most likely present. Further, this cyclic polling procedure involves both feeding forward the activities of neurons excited by the stimulus (corresponding to the primary repertoire), but also feeding back activities of higher-order groups that have sampled the primary repertoire and formed associations at the level of the secondary repertoire. Edelman argues that ``...the conscious state results from phasic reentrant signaling occurring in parallel processes that involve associations between stored patterns and current sensory or internal input.'' The word ``phasic'' refers to the fact that this polling of the neuronal groups occurs cyclically, and the word ``reentrant'' refers to the fact that information from higher cognitive centers is fed back to the primary modules so the contextualized information accessed in the secondary repertoire can be compared to the information being fed forward from the primary repertoire. Through these cycles, higher-order processing centers can identify ``multimodal associative patterns'', place incoming information in the complex context learned through prior experience, and quickly identify changes in inputs. 

Edelman's construction incorporates several of the core concepts I have described here. First, central to the hypothesis is the idea that small groups of neurons form specialized processors that identify a specific subset of features of input stimulus. As explained by Edelman, ``...each group has a limited set of characteristic spatiotemporal response patterns of firing as well as a characteristic set of connections to other groups.'' The mini-columns and columns identified by Mountcastle are clear candidates to form these basic, modular structures. We have emphasized the concept that transient neuronal ensembles form temporary coalitions to represent specific stimuli, and this idea is rooted in Edelman's theory. A single mini-column can represent a number of different stimuli, because the neurons therein can dynamically adapt their activity to form a variety of transient ensembles depending on the input. Each unique transient ensemble must produce a unique output so it can be identified as a unique pattern by other mini-columns in subsequent stages of processing. Related to this concept is the fact that the cortex comprises a very large number (nearly a billion) mini-columns. This allows the brain to recognize and correlate an enormous number of unique patterns across a multiple sensory modalities.

The second core concept emphasized by Edelman is the role of oscillations in the process of cognition. Edelman identified theta oscillations as a promising candidate for the cycles of activity during which the specialized processors (neuronal groups) are polled. On time scales much faster than theta oscillations, local groups of neurons interact and develop activities in response to stimulus. Then, with each cycle of theta activity, the information from many such groups is sampled, and higher-order patterns are identified, corresponding to identification of patterns at higher levels of abstraction. 

Third, the manner in which Edelman described the activity of neuronal groups during successive periods of theta oscillations contained the seeds of modern descriptions of neuronal activity in the framework of dynamical systems. In particular, Edelman described ``a continuous shifting pattern of associations'' occurring through the interplay between primary neuronal groups and the reentrant feedback from higher-order stages of processing. This ``continuous shifting pattern'' is highly reminiscent of the ``trajectories moving along heteroclinic orbits that connect saddle fixed points or saddle limit cycles in the system's state space'', as described by Rabinovich et al. \cite{ravo2001} nearly 25 years later. In the pictures described by Edelman and Rabinovich, neuronal groups are engaged in endless cycles of winnerless competition as new input is received and new feedback signals are generated.

Fourth, Edelman's model emphasized the central role of adaptability in forming associations between patterns observed repeatedly: ``...the selection of certain subgroups results in an alteration of the probability that these subgroups will be selected again upon a repeated presentation of a similar stimulus pattern.'' Incorporating the ideas of Hebb nearly 30 years earlier, Edelman assumed this alteration is ``a result of synaptic alteration'' so that ``connectivity is functionally altered.'' Twenty years later, the role of STDP and dendritic activity would be identified as a primary means of achieving these altered connections \cite{bipo1998,pome2001}. Edelman recognized the utility in these adapting functional networks for accomplishing associative memory that is content-addressable, emphasizing that within this model, ``Memory readout is not posed as a special problem; the \textit{process} does not differ from other forms of neuronal communication....'' Thus, unlike digital computers, information processing and memory access are not separate activities in neural systems.

Finally, the model proposed by Edelman had a place for a type of central control unit that could receive bottom-up input from primary neuronal groups, coordinate top-down feedback from higher-order associative centers to the primary groups, and establish coherent cycles in which this information exchange can occur. Edelman identified the thalamocortical complex as the primary candidate for achieving this system-wide coordination. In this architecture, modules at the system level are specialized for processing certain types of information (visual, olfactory, language, etc.), and each of these modules comprises many neuronal groups specialized for detailed  processing of this class of information. Thus, the hierarchical, modular architecture of the brain extends from the microscale of groups of neurons up to the sophisticated thalamocortical complex that coordinates system-wide activity. 

Edelman laid out multiple testable predictions that could be used to evaluate and falsify his theory. After forty years, his ideas have been refined, but not overturned. In designing hardware for general intelligence, we should expect similar principles to pertain, and we should incorporate device mechanisms to facilitate these operations at the local level of neuronal groups all the way up to the system level of the thalamocortical complex.

\subsubsection{Baars' Model of the Global Neuronal Workspace}
%Bernard Baars was the next to advance the concepts laid out by Edelman. Baars published his ideas in his 1988 book entitled ``A Cognitive Theory of Consciousness'' \cite{ba1988}, wherein the Global Neuronal Workspace was introduced.

Building on the concepts developed by Edelman, Bernard Baars introduced the concept of the global workspace in 1988 \cite{ba1988}. In this model, Baars describes a set of specialized experts vying for access to a shared chalkboard on which they can broadcast their messages for viewing by all the experts. In the most basic construction of Baars' model, the experts can all view the information on the chalkboard as well as write upon the chalkboard when granted access. In Baars' words, ``This simple model has only two theoretical constructs: a set of distributed specialized processors and a global workspace or `blackboard,' which can be accessed by a consistent set of specialists and that can, in turn, broadcast information to all others.'' (\cite{ba1988} pg 71) 

Each of the specialized processors is an expert in interpreting a certain type of information or extracting certain features from inputs. Anatomically, they correspond to minicolumns, columns, and brain regions, all specialized at different levels of hierarchy. A minicolumn may respond maximally to lines of a certain orientation, while a region (or collection of regions \cite{paper about the complex networks identifying faces}) may respond maximally to faces. At all levels of hierarchy, the thalamocortical complex controls which specialized processors have access to the global neuronal workspace (represented by the chalkboard in Baars' model) at a given moment. Once a specialized processor, or combination of mutually engaged processors, gains access to the global workspace, the information they are generating is shared broadly with the entire network of processing experts. All sub modules have access to the information globally broadcast and can use it to inform their activities. At any given moment, the information shared across the global workspace is associated with a train of thought, or a coherent cognitive sequence, enabling cognitive resources to be focused on the subject that has stimulated the dominant coalition of processors, thereby gaining that coalition access to the global workspace.

In the present context of informing the design of hardware for cognition, Baars' model reiterates two themes we have been discussing: 1) neuronal circuits must be highly adaptive and capable of transitioning between myriad dynamic ensembles as activity proceeds; and 2) the communication infrastructure must enable each specialized processor to readily communicate to and engage with many other processors all across the network with short communication pathways. We will return to these themes in Sec.\,\ref{sec:hardware} in the discussion of approaches to artificial hardware for cognition.

\subsubsection{The Role of Synchronization in Cognition}
By the early 2000s, the ideas of Edelman and Baars had been refined and supported by experimental evidence. We can get a snapshot on the understanding of cognition at that time by considering two review articles that appeared in Nature Reviews Neuroscience in 2001. 

In Ref.\,\cite{vala2001}, Varela et al. review ``the large-scale integration problem'': ``How does the brain orchestrate the symphony of emotions, perceptions, thoughts and actions that come together effortlessly from neural processes that are distributed across the brain?'' In tracing the operations of the brain to the manifestation in hardware, Varela et al. identify neural assemblies as the central construct. They define neural assemblies as ``distributed local networks of neurons transiently linked by reciprocal dynamic connections.'' Neural assemblies are central to understanding cognition because ``the emergence of a specific neuronal assembly is thought to underlie the operation of every cognitive act.'' With the term ``neural assemblies'', Varela et al. are not referring to the neuronal groups of Edelman or the specialized processors of Baars, but rather to the network of neurons that is transiently excited due to their collective relevance in processing a given stimulus. A neural assembly may be relatively localized, or it may span many modules across the network. Varela et al. describe two types of connections necessary to form these assemblies: one involves ``reciprocal connections within the same cortical area or between areas situated at the same level of the network.'' The other involves ``connections that link different levels of the network in different brain regions to the same assembly and embody the true web-like architecture of the brain.''

A central question regards the mechanisms by which these neural assemblies coordinate their activities to achieve information integration across the network. Varela et al. argue that two concepts are relevant to understanding information integration: 1) bottom-up and top-down activity; and 2) phase synchronization. Regarding the former, the authors clarify that ``Bottom-up and top-down are heuristic terms for what is in reality a large-scale network that integrates both incoming and endogenous activity; it is precisely at this level where phase synchronization is crucial as a mechanism for large-scale integration.'' The role of synchrony is elaborated in the context of visual binding, where the goal is to answer the question, ``how are the different attributes of an object brought together in a unified representation given that its various features\textemdash edges, colour, motion, texture, depth and so on\textemdash are treated separately in specific visual areas?'' The authors propose the answer, ``visual objects are coded by cell assemblies that fire synchronously...visual binding refers to the `local' integration of neuronal properties (that is, integration that takes place within neighbouring cortical areas, all specialized in the same modality), which allows the large-scale integration necessary for vision in the context of a complete cognitive moment. We argue that synchronization of neural assemblies is a process that spans multiple spatial and temporal scales in the nervous system.'' In this review I have emphasized the different roles of gamma and theta oscillations for simplicity, but Varela et al. (as well as many others \cite{stsa2000,budr2004,bu2006,sape1998,jeco2007,uhro2009}) emphasize that transient synchronization occurs across a continuum of spatial and temporal scales. Across these scales, the specific assemblies that are engaged adapt dynamically due to synaptic plasticity mechanisms \cite{abre2004,ab2008,xxx}, functional reconfiguration through inhibition \cite{robu2015}, and frequency-selective communication \cite{izde2003}. The authors acknowledge that information integration through coherent synchronization is not independent of concepts from dynamical systems: ``The transient nature of coherence is central to the entire idea of large-scale synchrony, as it underscores the fact that the system does not behave dynamically as having stable attractors, but rather metastable patterns\textemdash a succession of self-limiting recurrent patterns.''

In another 2001 review, Engel, Fries, and Singer offer additional insights into the role of oscillations and synchrony in top-down processing \cite{enfr2001}. These authors define top-down influences as ``intrinsic sources of contextual modulation of neural processing'' and argue that ``processing of stimuli is controlled by top-down influences that strongly shape the intrinsic dynamics of thalamocortical networks and constantly create predictions about forthcoming sensory events,'' echoing many of the concepts related to reentrant feedback discussed by Edelman. Engel et al. argue that ``Coherence among subthreshold membrane potential fluctuations could be exploited to express selective functional relationships...and these dynamic patterns could allow the grouping and selection of distributed neuronal responses for further processing.'' They emphasize that synchronization through the joint enhancement of response saliency can select and group subsets of neuronal responses for further joint processing. So, synchronization can be used to encode information about the relatedness of neural signals....'' Yet the main thesis of Engel et al. regards the role of top-down processing. They argue that ``top-down factors can lead to states of `expectancy' or `anticipation' that can be expressed in the temporal structure of activity patterns before the appearance of stimuli.'' Further, ``not only changes in discharge rate, but also changes in neural synchrony, can be predictive in nature.'' Not only does phase coherence of ongoing synchronized oscillations play a predictive role, but synchronization of first spikes generated by two neurons upon presentation of a stimulus can enable the network to anticipate the contents of the full stimulus. From the perspective of Engel, Fries, and Singer, top-down state modulation plays a crucial role in preparing the network to contextualize new information and in adapting the network in the presence of changing stimulus. These ideas are consistent with but extend beyond the framework of reentrant, phasic processing as described by Edelman.


\subsubsection{Communication Through Coherence}

By 2015, Fries had elaborated these ideas still further. In Ref.\,\cite{fr2015} he argues, ``...dynamic changes in synchronization can flexibly alter the pattern of communication. Such flexible changes in the brain's communication structure, on the backbone of the more rigid anatomical structure, are at the heart of cognition.'' This perspective illustrates the significance of the brain's ability to adapt its single structural network into myriad functional networks under the influence of ever-changing information. Fries asserts the significance of coherence in communication based on the idea that ``Inputs that consistently arrive at moments of high input gain benefit from enhanced effective connectivity. Thus, strong effective connectivity requires rhythmic synchronization....In the absence of coherence, inputs arrive at random phases of the excitability cycle and will have a lower effective connectivity.'' Most crucially, ``A postsynaptic neuronal group receiving inputs from several different presynaptic groups responds primarily to the presynaptic group to which it is coherent. Selective communication is implemented through selective coherence.''

\subsubsection{Experimental Progress Identifying the Mechanisms of Cognition}
With modern brain imaging techniques and clever experiments, it has become possible to test aspects of the global neuronal workspace theory. Stanislas Dehaene has conducted some of these experiments and found clear correlations between the experience of becoming aware of a stimulus and the physical activity corresponding to that experience, and his 2015 book ``Consciousness and the Brain'' provides an accessible summary of the field for the uninitiated. Dehaene describes how a ``priming'' stimulus (such as an image of a number or a word) can be shown to excite characteristic activity in a localized brain region, and even shown to affect the activities of other regions. But only when the local activity leads to a cascade of global activity does the subject report becoming aware of the stimulus. Subconscious processing of information stays local and decays rapidly upon removal of the stimulus, while conscious processing spans the network and can remain active long after the stimulus is no longer present.

This emerging picture, supported by a great deal of experimental evidence, has a great deal in common with the ideas of Edelman as well as Baars: local, specialized processors deal with information of a specific type, and the full comprehension of this information can only occur across wider regions of the network that sample input from many specialized processors. This information is integrated across longer temporal scales, and on these longer temporal and spatial scales, the architecture of the network enables the information to be shared broadly and compared to prior learned models of the world through a combination of feed-forward and feed-back connections spanning the hierarchy of the network. Dehaene refers to these distinct patterns of activity that occur when a stimulus becomes conscious as avalanches. Small avalanches that remain confined to a local region remain unconscious. Large avalanches that induce activity across the global neuronal workspace lead to conscious awareness. In describing recordings of such avalanches, Dehaene writes, ``Distant brain regions also became tightly correlated: the incoming wave peaked and receded simultaneously in all areas, suggesting that they exchanged messages that reinforced one another until they turned into an unstoppable avalanche. Synchrony was much stronger for conscious than for unconscious targets, suggesting that correlated activity is an important factor in conscious perception.'' \cite{de2015} In this description, Dehaene ties together several of the concepts I have emphasized here, including neuronal avalanches, the role of synchrony, and information integration across space and time. The model of cognition championed by Dehaene is coherent with concepts coming from Edelman, Baars, Engel, Fries, Singer, and others. The models have been refined over time, and much remains unknown, but the coarse outline has stood the test of time. 


\subsubsection{Lessons for Design of Hardware for General Intelligence}
The concepts presented in this section represent our guideposts for designing hardware capable of general intelligence. Most importantly, an intelligent system must be able to efficiently exchange information across many scales of space and time. This sounds obvious, but in our present computing hardware contains many pinch points where information going to very different destinations is serialized, and communication events simply wait in line. This method of information exchange is antithetical to the way computing occurs in the brain.

More specifically, at a network level the presence of a new stimulus must rapidly lead to the establishment of new synchronized states. This can only occur if information can be efficiently communicated across the entire network. Local assemblies must respond to the new stimulus on the time scale of gamma oscillations, and large numbers of these assemblies must be sampled on the time scale of theta oscillations. Each local assembly must be able to broadcast information to many recipients to achieve efficient bottom-up communication to higher-level processing centers, and these centers must also be able to broadcast their activity across the network to achieve efficient top-down communication and functional reorganization. Many connections are in close spatial proximity, but many are not. The white-matter wiring and the architecture of the constituent modules cannot be separated from the cognitive functions they perform \cite{bosp2015}. The hardware infrastructure for achieving this communication must not introduce competition for bandwidth on a shared communication infrastructure, and it is for this reason that all communication channels must have dedicated axonal fibers. Regardless of the physical medium employed for signaling, if the communication infrastructure is shared and requires serialization, there will be a scale beyond which specialized processors cannot be adequately sampled, and the processing hierarchy will be constrained.

The prior argument informs the design of the communication network, and at the device level, quantities as simple as time constants play an important role in determining the frequencies at which synchronization can occur and the durations over which coherence of activity can be maintained. The relevant frequencies span many orders of magnitude, and the ability to achieve a wide range of time constants with accurate control by design appears highly advantageous for enabling the synchronization-based exchange of information across the network. Additionally, devices that perform nonlinear transfer functions and make use of the temporal domain with spiking signals will be particularly equipped to harness the insights gained from neuroscience. 

Finally, it is important to include large numbers of primitive elements to provide the broad repertoire necessary for general intelligence, a fact that was not lost on Ram\'{o}n y Cajal, Mountcastle, or Edelman. In this regard both size and power dissipation must be taken into account. Neurons must be small enough that signals can propagate between them during the period of network oscillations without being limited by communication delay, and the energy per synaptic event must be low enough that networks with trillions of synapses can function with tolerable power density and net consumption. Section \ref{sec:hardware} considers several approaches to constructing hardware toward these ends.

\vspace{3em}
columns = modules = specialized processors = subconscious experts

\vspace{3em}
\cite{ed1978} polling of groups, similar to Baars; requires efficient communication

\vspace{3em}
as evolution proceeded, animals developed larger brains with more columns as well as greater interconnectivity. This is consistent with the picture of path length on a random graph (see ed1978 pg 71)

\vspace{3em}
ed1978 uses terms ``group'' and ``repertoire'' ``central states''

\vspace{3em}
role of oscillations, clusters/avalanches, dynamic states in Edelman's model

\vspace{3em}
Compare neural to concurrent computing