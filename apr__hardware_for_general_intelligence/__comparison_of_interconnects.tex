\subsection{\label{sec:comparison_of_interconnects}Comparison of Interconnect Technologies}

\subsubsection{Copper Interconnects}
The majority of contemporary neuromorphic computing is based on silicon microelectronics, and for much of the community, the objective is not to answer the question, ``What hardware is optimal for neuromorphic computing?'' but rather to answer the question, ``What forms of neuromorphic computing can I accomplish with the hardware that I have?'' 

\vspace{3em}
The primary challenge for adapting digital hardware for neural applications is one is attempting to use a system with a certain graph structure and set of information processing operations to behave as a system with a completely different graph structure and a completely different set of information processing operations. The digital computing machine being utilized is Turing complete, so it can accomplish the task, but because the structures and operations are entirely unrelated, it is extremely inefficient.

\vspace{3em}
In much of the literature, it is assumed that computer architecture will not change in future generations of hardware, so the questions become whether specific components will be replaced with alternative devices. One can consider whether photonic interconnects make sense in specific locations, such as for on-chip interconnects, between processor and DRAM modules on a common, between boards through back plane, or between servers in a rack. Each decision can be made separately. 

\vspace{3em}
include a figure regarding chip, board, back plane, server rack. perhaps Fig. 4 of \cite{husz2003}

\vspace{3em}
The needs of neural information processing are precisely where conventional interconnection networks perform the worst.

\vspace{3em}
Figure F.19 of \cite{hepa2012} may be helpful. From the same reference, this expression for latency:
\begin{equation}
\label{eq:switching_network_latency}
\begin{split}
\mathrm{Latency} & = \mathrm{Sending}\,\mathrm{overhead} \\
&+T_{\mathrm{LinkProp}}\times(d+1)+(T_{\mathrm{r}}+T_{\mathrm{a}}+T_{\mathrm{s}})\times d \\
&+\frac{(\mathrm{Packet})+(d\times\mathrm{Header})}{\mathrm{Bandwidth}} \\
&+\mathrm{Receiving}\,\mathrm{overhead}
\end{split}
\end{equation}
pg F-52.
``Routing, arbitration, and switching can impact the packet latency of a loaded network by reducing the contention delay experienced by packets.'' \cite{hepa2012}
``At higher applied loads, latency increases exponentially, and the network approaches its saturation point as it is unable to absorb the applied load, causing packets to queue up at their source nodes awaiting injection.'' \cite{hepa2012}
This traffic/speed trade off means that the frequencies of synchronized oscillations that can be sustained by a neuronal ensemble depends on how many neurons are participating in the ensemble. But it is precisely the synchronized exchange of information across the network through simultaneous communication of many neurons in large, transient ensembles that is the essence of cognition, as we have explored in Sec.\,\ref{sec:neural_systems}. If we have the liberty to design hardware for general intelligence from scratch, it would be unwise to build this communication bottleneck into the hardware. It is best avoided if each neuron has a dedicated axonal arbor that reaches all of its synaptic connections independently, and this fan-out is likely impossible to achieve with normal-metal interconnects. Can superconductors do better?

\subsubsection{\label{sec:superconducting_interconnects}Superconducting Interconnects}
In designing neural systems 

\paragraph{Passive Superconducting Fan Out}
Let us first consider the case where the output from each neuron does not need to switch a JJ at each receiving synapse, but rather the sum of multiple synaptic inputs to each neuron switches a neuronal thresholding junction. I refer to this as the passive superconducting fan out scenario. Suppose this thresholding junction is biased such that the net input must be equal to $I_0$ in order to drive the JJ above $I_{\mathrm{c}}$. Also suppose each neuron's threshold JJ produces a current $2I_0$ when it reaches threshold, consistent with the usual case of superconducting electronics that fan-out of two can be supported. Consider an ensemble of neurons in which each neuron makes $n_{\mathrm{s}}$ synaptic connections. For simplicity, make the favorable assumption that the output JJ can produce $2I_0$ regardless of $n_{\mathrm{s}}$. In practice, this will be very difficult for large $n_{\mathrm{s}}$, as the total inductance of the fan-out tree, $L_{\mathrm{T}}$, will become large, and $\Phi_0/L_{\mathrm{T}}$ will become small. Also make the favorable assumption that the fan-out tree can evenly distribute current to all synapses, regardless of spatial location. Four problems are associated with this approach to interconnection. We discuss them here in order of severity.

%first problem: nearly all synapses must fire to drive neuron to threshold
Within this simplified passive fan out model, each synapse receives current equal to $2I_0/n_{\mathrm{s}}$. We then ask: how many synapses must fire within an integration time to cause the neuron to switch? We have stated that the neuron requires $I_0$ to switch, so $n_{\mathrm{s}}/2$ synapses must fire concurrently to drive the neuron to threshold. This identifies the first problem with passive electronic fan out between superconducting electronic neurons: order $n_{\mathrm{s}}$ synapses must all be active at once to drive a neuron to threshold. In biological neurons, this number is closer to order $\sqrt{n_{\mathrm{s}}}$ \cite{}.The problem with requiring $\mathcal{O}(n_{\mathrm{s}})$ synapses to all fire at once is that only very strong stimulus leads to persistent activity. If a neuron is to fire, at least half of its input synapses must be active at a given moment, and if this condition is not met, activity dies out. As we have discussed in the context of neuronal avalanches, an important aspect of dynamical activity in neural systems is that any given neuron should have the potential to generate neuronal avalanches, but this cannot occur if half of each neuron's input synapses must be active simultaneously to perpetuate activity. We wish to be able to control and choose the number of synapses that drive a neuron to threshold. And, we wish to be able to make some of a neuron's output synapses strong without requiring that others become weaker. We can accomplish this if we have gain at each synapse, which occurs if each synapse has a JJ that switches with each synaptic firing event.

%second problem: synaptic time constant is the same as action potential duration
Another problem resulting from passive fan out relates to post-synaptic time constants. If the signal received by each synapse is simply the current produced by a neuron's output JJ, then the duration of the post-synaptic potential is the same as the duration of the action potential. This post-synaptic time constant is responsible for the integration time of the neuron, and it is desirable to be able to choose this value over a broad range across the synapses in the network. Some synapses can forget rapidly, while other must retain information over an extended period of time. If the post-synaptic time constant is the same as the duration of the action potential, the neuron does not perform integration, and all inputs must be precisely synchronized in time. Immediately above, we identified the problem that $\mathcal{O}(n_{\mathrm{s}})$ synapses must fire to drive a neuron to threshold. If the synapses do not fire at precisely the same time, and instead experience jitter of even a fraction of the duration of the action potential, then even more synapses are required to fire, and it may become impossible to drive the neuron to threshold without extremely precise timing. In a system where a single flux-quantum represents an action potential, this requires timing precision on the order of 10\,ps, which is extremely difficult to maintain across a wide network of spatially distributed neurons. In some cases it may be possible to use this as a mechanism to induce precise synchronization, but it is not desirable to have it be a hard requirement at every neuron. In designing hardware for general intelligence, we must be able to engineer the action potential duration and synaptic time constants independently. We can accomplish this if we have gain at each synapse.

%third problem: as you add connections, inductance of axonal tree grows, and the amount of current generated by neuron's output JJ decreases
A third problem resulting from passive fan out relates to the distribution of current across an axonal tree in the case where neurons make a large number of synaptic connections. With passive superconducting wires, current is divided among the synaptic destinations by controlling the inductance of the paths to all the synapses. In several such circuit designs, the synaptic weight is represented by the amount of current reaching each synapse \cite{scdo2018,xxx,yyy}. In such cases, the maximum synaptic weight is equal to the total amount of current reaching the synapse, and a variable circuit element can discard some of this current to achieve a lower synaptic weight. The total output current of a neuron's thresholding junction will be equal to $\Phi_0/L_{\mathrm{T}}$ based on fundamental Josephson physics. The tree inductance, $L_{\mathrm{T}}$, is presumed to result primarily from the wiring carrying the currents. With standard Nb wires, the inductance per square is roughly 500\,fH/$\square$. As networks grow in scale and connections become more distant, the current generated by a neuron's output JJ decreases. We are faced with a fan-out/current-supply trade-off. But we require exactly the opposite: as we add connections, we must supply more current to drive them. The inductance of the tree can be reduced by using wider wires, but then more area is consumed, and connections become further away. Such wire width/wire length trade-offs are not unique to superconductors and have been investigated thoroughly in the context of CMOS electronics \cite{}.

%fourth problem: as connections become distant, axonal tree is difficult
Finally, a fourth problem arises when one attempts to utilize passive superconducting fan out to connect many neurons in a complex network. This challenge is related to achieving an even distribution of current across many synapses located across a network. If all connections are equidistant, the can use the same wire length, and because they are wired in parallel, the total inductance of the tree will be equal to the inductance of a single connection. However, as discussed in Sec.\,\ref{sec:spatial_structure_of_neural_systems}, small-world networks and modular, hierarchical networks require long-range connections to maintain short network path lengths and efficient cross-network communication. In the case of passive superconducting fan-out, these distant synaptic connections require the same amount of current as proximal connections, and therefore the wires making distant connections must be wider and thicker to achieve the same inductance. Because there is a maximum practical thickness for wires produced in a standard damacene process, this results in a scaling trend where the width of wires is linearly proportional to the distance the wire must cover. Long-range connections thus incur significant wiring area when passive superconducting lines are used. Related to this problem is the challenge of devising routing and layout algorithms that place all the nodes and instantiate the fan out tree so as to equalize the inductance of all connections. These challenges become more serious as the scale of the network grows.

To summarize, there are at least four problems that arise if passive fan-out is to be utilized: 1) $\mathcal{O}(n_{\mathrm{s}})$ synapses must fire simultaneously; 2) the post-synaptic time constant is same as action potential duration; 3) distributing currents across synapses requires inductive dividers, and as more synaptic connections are added, the current generated by each neuron decreases, making it more difficult to drive large numbers of synapses; and 4) with increasing numbers of neurons, the design and implementation of the axonal current distribution tree becomes significantly, and very wide wires with low inductance are required to ensure distant connections receive sufficient current.
%but as different synapses are placed different distances away, it becomes difficult to evenly distribute current, and distant synapses get very small amounts of current, which makes it hard to achieve small-world networks. 

\paragraph{Active Superconducting Fan Out}
%needs much more attention; address digital, active is used exclusively, fanout of of two, trees
%note that an axon is an active transmission line: the sum of the currents reaching the synaptic terminals is not equal to the current generated by the axon hillock; charge is brought into the line from the extracellular medium
In the case of active superconducting fan out, each synapse comprises at least one JJ that is driven above $I_{\mathrm{c}}$ when a synaptic firing event occurs. Such a scenario solves the problems identified above in the case of passive superconducting fan out, yet new challenges arise. In the active case, each neuron's output JJ must switch a JJ at each receiving synapse. Again, we assume a current of $I_0$ is required to do so. Consider two scenarios. First, the current from the neuron is split evenly by a passive, superconducting tree. A typical junction used for digital logic will produce 100\,\textmu A upon switching, meaning the inductance of the tree must stay below 10\,pH. Typical niobium wires have 500\,fH per square and must be approximately 1\,\textmu m wide to carry this current, so typical distances from a neuron to its synapses must be about 20\,\textmu m.

\subsubsection{Photonic Interconnects}
Fan out is limited if electrons are used for communication. The basic problem is the same whether superconducting lines of normal conductor are utilized: if downstream devices must be driven by charge, there is a fan out problem. In the case of transistors, capacitors must be charged. In the superconducting domain, JJs must be driven above $I_{\mathrm{c}}$. In both cases, it is impractical to source enough charge to drive many downstream elements, and fan out is limited. By contrast, with photons and single-photon detectors, the problem is tractable. If each downstream element requires $\mathcal{O}(1)$ photon for activation, and the response is identical whether one or more photons is received, a single optical source can easily produce enough photons to drive thousands of synaptic elements. Ten thousand near-infrared photons contain a femtojoule of energy. Further, it appears possible to design and implement a passive photonic routing infrastructure that can route thousands of photons to thousands of destinations, even if those destinations are located in distant regions of a multi-module network. All routing is passive, and no memory is required to keep track of destination addresses.

\vspace{3em}
With photonic routing, distant connections still require wider wires (waveguides), but for a connection that is 10x further away, we may require a wire that is 2x the width to maintain the same loss (index contrast, Rayleigh scattering), whereas for passive superconducting we require 10x the width to maintain the same inductance (this compares signal amplitudes).

\paragraph{Photonic Communication in Digital Systems}

\paragraph{Photonic Communication in Neural Systems}

\vspace{3em}
Optical interconnects have been considered for decades in the context of digital computing with processors and memory chips communicating over copper traces on circuit boards. In that context, many of the trade offs are well understood. For example, a key role of the interconnect is to allow processors to access DRAM located on a different chip manufactured in a different process. A primary source of latency in memory access is related to the DRAM devices themselves and the fact that many bits must share a common access line to keep costs low. It has been argued that optical communication does not help reduce latency in memory access because it is limited by the DRAM devices themselves, not by the communication time \cite{husz2003}. However, in the context of large-scale neural computing that concerns us here, a primary goal is to eliminate the separation of processing and memory so that the memory mechanisms discussed in Sec.\,\ref{sec:memory} can be realized directly with physical devices implemented in hardware rather than emulated with a Turing machine. In this case, the demand for efficient communication is increased, while many of the arguments against photonic communication at the local scale do not apply. Practical considerations such as manufacturing cost are still of central relevance.

\vspace{3em}
It is difficult to achieve graph structures with highly reciprocal and reentrant connectivity using free-space optics or MZI networks. These technologies are more useful for feed-forward networks with applications to matrix-vector multiplication and deep learning.
 

\vspace{3em}
There is ``little doubt that interconnects are now and will be increasingly a major limitation on information processing systems. There is also little doubt that the physics of optics offers potential solutions.'' \cite{mi2009}

\vspace{3em}
``There is more motivation to implement a potentially risky and costly new technology when it provides a fewature that cannot be obtained in the old one.'' \cite{husz2003}

Global clock distribution across the backplane is an application space where optical interconnects may be useful. While neural systems will not have the same architecture, the general function of synchronizing signals broadcast over large regions of the network is still present. It is clear that optical communication has significant advantages for this function. However, in neural systems there is not a single clock, and instead many neurons spread throughout the network must be able to generate and distribute signals to nearby and distant targets, thus motivating compact light sources localized with each neuron. Further, the ability of a neuron to communicate at a given moment must not depend on the total number of other neurons communicating at that moment, otherwise traffic-induced latency occurs, which limits the network's ability to rapidly adapt to changing stimuli. This latency depends exponentially on network traffic (see hepa2012 discussion above). The question must be made quantitative. How many neurons can participate in oscillations at a given frequency if limited by switching network latency?

Analyze with \ref{eq:switching_network_latency} as well as literature examples. 

\vspace{3em}
%On-chip silicon photonic signaling and processing: a review
\cite{walo2018}

\vspace{3em}
In any photonic routing architecture, the on-wafer connectivity will be limited by the number of waveguide planes implemented in the fabrication process. A limit to this number will result from the fact that oxide layers have material stress, which causes the wafer to bow, and hinders processing. The effect is worse for larger wafers. Thus, a tradeoff between wafer size and number of planes will establish the barrier of scaling. This appears as a practical limitation, but ultimately it is a physical limitation dictated by the atomic properties of silicon and oxygen which are set by the constants of nature. Many such limits that appear as practical are actually physical limits and therefore will result in asymptotic saturation of technological capabilities that cannot be surpassed with further investment of resources.

\vspace{3em}
-\cite{juya1996} ``The principal reason for using optics in neural networks, is that the very large interconnection or fan-in fan-out, of the order of $10^3$ to $10^6$, is quite impractical for VLSI technology because it uses a unique discrete channel for each input or output \cite{caki1989}. On the contrary, we may conceive that an optical neural network is not significant unless each neuron has more that 1000 interconnections with other neurons.''
-``The architecture of a neural network is characterized by interconnection and non-linear operation. In principle, optics is naturally suitable for implementing the interconnection. On the other hand, the non-linear operation may be implemented by electronic means, including digital computers.'' \cite{juya1996}
-1) The velocity of optical signals is independent of the number of interconnections; 2) optical signals are immune to mutual interference effects; 3) Optical signals can propagate in three-dimensional free space; 4) the interconnection can be altered properly using spatial light modulators; and 5) optical signals can be easily converted into electronic signals. \cite{juya1996}

\vspace{3em}
As Goodman pointed out in 1985, ``quote from Goodman's paper'' The benefits of light for fan-out can be difficult to harness in free space. We may expect the situation to improve with on-chip waveguides. ``Two beams of light, unlike a pair of current-carrying wires, can cross without affecting each other.'' \cite{abps1987} At the same time that free-space optical neural computers were being developed based on holographic memory, the field of integrated photonics was emerging.

\vspace{3em}
Attempting to establish synaptic weights by precisely controlling the phase of optical signals for each synaptic connection is a problematic foundation in one intends to construct a cognitive system at the scale of the brain. It is tempting to use phase for modulation or frequency for multiplexing, but physical features are are not well-equipped to achieve basic synaptic and neuronal functions.

\vspace{3em}
Waveguide planes are an aspect of hardware, while network layers are a concept from architecture. To minimize waveguide crossings and interplanar transitions, lower planes are used for local connections, and upper planes are used for distant connections. Groups of planes corresponding to certain communication lengths scales are identified by the material composition of the waveguides in the planes as this material composition dictates the index contrast and therefore loss as well as waveguide size. We expect to utilize at least three materials for three levels of this communication hierarchy: silicon, silicon nitride, and low-index-contrast glass. The loss/density trade off is a primary affector of Rentian scaling potential. The tuning along this loss/density curve is continuous with SiO$_x$N$_y$ by varying $x$ and $y$. Mature material stacks will result from a combination of considerations at the holistic, system level, including power consumption, computational capabilities, and cost/feasibility of fabrication.


\subsubsection{Photonic Interconnects with Superconducting Receivers}
Considerations pertinent to photonic interconnects with superconducting receivers in neural systems are related to considerations pertinent to photonic interconnects with semiconducting receivers in digital systems \cite{mi2009,mi2017}, although distinct considerations become relevant. It may be possible that photonic communication is deemed suitable in one application space at the chip, wafer, and systems scales, while it is not suitable in another application space until the system scale. 

\vspace{3em}
There are several reasons we may expect photonic interconnects to be present in high-performance neural systems utilizing superconducting electronics and operating at low temperature:
\begin{itemize}
\item The energy per detection event due to the detector alone is at least two orders of magnitude smaller than with a photodiode
\item This energy efficiency is further improved because the detectors can perceive a single photon, and order five to 10 photons are required to overcome noise associated with Poisson statistics
\item Because of the manner in which neural systems utilize space and time, sources and detectors at neurons and synapses need not operate nearly as fast as in digital systems (100\,MHz firing rates would be extremely fast compared to biological systems) but they must operate over long distances (the 1\,cm spatial scale of a processor is very small for a complex neural system)
\item Operating at low temperature makes silicon light sources a viable candidate. Such sources can have extremely low capacitance, and the potential limits of internal quantum efficiency remain to be determined. But the primary benefit of silicon light sources is not performance, but rather process simplicity, which leads directly to cost reduction and economic viability.
\item Each neuron is itself a complex processor occupying at least 100\,\textmu $\times$ 100\,\textmu, and likely as big as a millimeter or two on a side, so if photonics is utilized for communication only between neurons (and not in the synaptic and dendritic processing occurring within each neuron), then the crossover condition where photonic communication becomes advantageous is satisfied. Miller calculates this to be on the order of 50\,\textmu m \cite{mi2009}.
\end{itemize}

\subsubsection{Summary}
It is difficult to conceive of hardware that can achieve the graphs we desire for neuromorphic computing. One can envision a network of copper wires connecting CMOS neurons and synapses, but this would not function due to the capacitance arguments presented above. CMOS neurons are invariably connected by a shared switching network, and as we have argued, this leads to connectivity/speed trade offs that limit performance long before the scale that concerns us here. Superconducting interconnects do not fare better. Connecting many spiking neurons with superconducting interconnects requires active transmission lines that negate any power benefits and lead to formidable wiring and current biasing challenges. 

Optics may be promising to overcome these limitations, but in free space, routing is difficult, particularly with high fan-out and recurrent network structures. Fiber optics are far too bulky to be the only means by which neurons are connected locally, but on-chip waveguides are appealing. They are entirely passive, do not experience charge-based parasitics, and may enable neurons to have independent, dedicated axonal connections to each synapse. The size of the waveguides is limited by the wavelength of light to about 1\,\textmu m in the two transverse directions, but this is does not make the interconnection network unworkably large, provided multiple planes of compact dielectric waveguides can be employed. Frequency multiplexing may lead to effective reduction in spatial consumption by a factor of 10 or so, but such an approach puts an extra burden on light sources as well as spectral filters which generally need to be tuned. Using frequency as the only means of uniquely identifying each neuron greatly limits connectivity, but using frequency multiplexing in addition to a large number of independently routed waveguides may reduce network size, provided the source and filter challenges can be met. If these challenges cannot be overcome, monochromatic neurons each with an independent axonal tree still appear promising. Yes, they are large, particularly compared to biological neurons. But as we have argued, it is a neuron's area divided by the speed of signaling that affects network performance. With this particular connection network, signals can propagate at the speed of light across entire wafers, in free space between wafers, and over fiber to distant regions of the network. This is the reasoning that leads us to consider a photonic connection infrastructure most promising for establishing the recurrent, modular, hierarchical, small-world graphs necessary for a cognitive  architecture. 