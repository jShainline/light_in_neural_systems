\subsection{Comparison of Interconnect Technologies}

\subsubsection{Copper Interconnects}
The majority of contemporary neuromorphic computing is based on silicon microelectronics, and for much of the community, the objective is not to answer the question, ``What hardware is optimal for neuromorphic computing?'' but rather to answer the question, ``What forms of neuromorphic computing can I accomplish with the hardware that I have?'' 

\vspace{3em}
The primary challenge for adapting digital hardware for neural applications is one is attempting to use a system with a certain graph structure and set of information processing operations to behave as a system with a completely different graph structure and a completely different set of information processing operations. The digital computing machine being utilized is Turing complete, so it can accomplish the task, but because the structures and operations are entirely unrelated, it is extremely inefficient.

\vspace{3em}
In much of the literature, it is assumed that computer architecture will not change in future generations of hardware, so the questions become whether specific components will be replaced with alternative devices. One can consider whether photonic interconnects make sense in specific locations, such as for on-chip interconnects, between processor and DRAM modules on a common, between boards through back plane, or between servers in a rack. Each decision can be made separately. 

\vspace{3em}
include a figure regarding chip, board, back plane, server rack. perhaps Fig. 4 of \cite{husz2003}

\vspace{3em}
The needs of neural information processing are precisely where conventional interconnection networks perform the worst.

\vspace{3em}
Figure F.19 of \cite{hepa2012} may be helpful. From the same reference, this expression for latency:
\begin{equation}
\label{eq:switching_network_latency}
\mathrm{Latency} = \mathrm{Sending}\,\mathrm{overhead}+T_{\mathrm{LinkProp}}\times(d+1)+(T_{\mathrm{r}}+T_{\mathrm{a}}+T_{\mathrm{s}})\times d+\frac{(\mathrm{Packet})+(d\times\mathrm{Header})}{\mathrm{Bandwidth}}+\mathrm{Receiving}\,\mathrm{overhead}
\end{equation}
pg F-52.
``Routing, arbitration, and switching can impact the packet latency of a loaded network by reducing the contention delay experienced by packets.'' \cite{hepa2012}
``At higher applied loads, latency increases exponentially, and the network approaches its saturation point as it is unable to absorb the applied load, causing packets to queue up at their source nodes awaiting injection.'' \cite{hepa2012}
This traffic/speed trade off means that the frequencies of synchronized oscillations that can be sustained by a neuronal ensemble depends on how many neurons are participating in the ensemble. But it is precisely the synchronized exchange of information across the network through simultaneous communication of many neurons in large, transient ensembles that is the essence of cognition, as we have explored in Sec.\,\ref{sec:neural_systems}. If we have the liberty to design hardware for general intelligence from scratch, it would be unwise to build this communication bottleneck into the hardware. It is best avoided if each neuron has a dedicated axonal arbor that reaches all of its synaptic connections independently, and this fan-out is likely impossible to achieve with normal-metal interconnects. Can superconductors do better?

\subsubsection{\label{sec:superconducting_interconnects}Superconducting Interconnects}
It is not explicitly necessary that the output of each neuron be able to switch a junction at a downstream synapse, but it is advantageous from an information processing viewpoint. Otherwise, interactions between synaptic events are limited to the few-picosecond duration of an SFQ pulse, and this restricts a neuron's ability to process information across a broad range of time scales. To get a feel for possible fan-out capacity, let us assume neuron's output JJ must switch a JJ at each receiving synapse and that 10\,\textmu A is required to do so. Consider two scenarios. First, the current from the neuron is split evenly by a passive, superconducting tree. A typical junction used for digital logic will produce 100\,\textmu A upon switching, meaning the inductance of the tree must stay below 10\,pH. Typical niobium wires have 500\,fH per square and must be approximately 1\,\textmu m wide to carry this current, so typical distances from a neuron to its synapses must be about 20\,\textmu m.

\subsubsection{Photonic Interconnects}
\vspace{3em}
Optical interconnects have been considered for decades in the context of digital computing with processors and memory chips communicating over copper traces on circuit boards. In that context, many of the trade offs are well understood. For example, a key role of the interconnect is to allow processors to access DRAM located on a different chip manufactured in a different process. A primary source of latency in memory access is related to the DRAM devices themselves and the fact that many bits must share a common access line to keep costs low. It has been argued that optical communication does not help reduce latency in memory access because it is limited by the DRAM devices themselves, not by the communication time \cite{husz2003}. However, in the context of large-scale neural computing that concerns us here, a primary goal is to eliminate the separation of processing and memory so that the memory mechanisms discussed in Sec.\,\ref{sec:memory} can be realized directly with physical devices implemented in hardware rather than emulated with a Turing machine. In this case, the demand for efficient communication is increased, while many of the arguments against photonic communication at the local scale do not apply. Practical considerations such as manufacturing cost are still of central relevance.

\vspace{3em}
It is difficult to achieve graph structures with highly reciprocal and reentrant connectivity using free-space optics or MZI networks. These technologies are more useful for feed-forward networks with applications to matrix-vector multiplication and deep learning.
 

\vspace{3em}
There is ``little doubt that interconnects are now and will be increasingly a major limitation on information processing systems. There is also little doubt that the physics of optics offers potential solutions.'' \cite{mi2009}

\vspace{3em}
``There is more motivation to implement a potentially risky and costly new technology when it provides a fewature that cannot be obtained in the old one.'' \cite{husz2003}

Global clock distribution across the backplane is an application space where optical interconnects may be useful. While neural systems will not have the same architecture, the general function of synchronizing signals broadcast over large regions of the network is still present. It is clear that optical communication has significant advantages for this function. However, in neural systems there is not a single clock, and instead many neurons spread throughout the network must be able to generate and distribute signals to nearby and distant targets, thus motivating compact light sources localized with each neuron. Further, the ability of a neuron to communicate at a given moment must not depend on the total number of other neurons communicating at that moment, otherwise traffic-induced latency occurs, which limits the network's ability to rapidly adapt to changing stimuli. This latency depends exponentially on network traffic (see hepa2012 discussion above). The question must be made quantitative. How many neurons can participate in oscillations at a given frequency if limited by switching network latency?

Analyze with \ref{eq:switching_network_latency} as well as literature examples. 

\vspace{3em}
%On-chip silicon photonic signaling and processing: a review
\cite{walo2018}

\vspace{3em}
In any photonic routing architecture, the on-wafer connectivity will be limited by the number of waveguide planes implemented in the fabrication process. A limit to this number will result from the fact that oxide layers have material stress, which causes the wafer to bow, and hinders processing. The effect is worse for larger wafers. Thus, a tradeoff between wafer size and number of planes will establish the barrier of scaling. This appears as a practical limitation, but ultimately it is a physical limitation dictated by the atomic properties of silicon and oxygen which are set by the constants of nature. Many such limits that appear as practical are actually physical limits and therefore will result in asymptotic saturation of technological capabilities that cannot be surpassed with further investment of resources.


\subsubsection{Photonic Interconnects with Superconducting Receivers}
Considerations pertinent to photonic interconnects with superconducting receivers in neural systems are related to considerations pertinent to photonic interconnects with semiconducting receivers in digital systems \cite{mi2009,mi2017}, although distinct considerations become relevant. It may be possible that photonic communication is deemed suitable in one application space at the chip, wafer, and systems scales, while it is not suitable in another application space until the system scale. 

\vspace{3em}
There are several reasons we may expect photonic interconnects to be present in high-performance neural systems utilizing superconducting electronics and operating at low temperature:
\begin{itemize}
\item The energy per detection event due to the detector alone is at least two orders of magnitude smaller than with a photodiode
\item This energy efficiency is further improved because the detectors can perceive a single photon, and order five to 10 photons are required to overcome noise associated with Poisson statistics
\item Because of the manner in which neural systems utilize space and time, sources and detectors at neurons and synapses need not operate nearly as fast as in digital systems (100\,MHz firing rates would be extremely fast compared to biological systems) but they must operate over long distances (the 1\,cm spatial scale of a processor is very small for a complex neural system)
\item Operating at low temperature makes silicon light sources a viable candidate. Such sources can have extremely low capacitance, and the potential limits of internal quantum efficiency remain to be determined. But the primary benefit of silicon light sources is not performance, but rather process simplicity, which leads directly to cost reduction and economic viability.
\item Each neuron is itself a complex processor occupying at least 100\,\textmu $\times$ 100\,\textmu, and likely as big as a millimeter or two on a side, so if photonics is utilized for communication only between neurons (and not in the synaptic and dendritic processing occurring within each neuron), then the crossover condition where photonic communication becomes advantageous is satisfied. Miller calculates this to be on the order of 50\,\textmu m \cite{mi2009}.
\end{itemize}

\subsubsection{Summary}
It is difficult to conceive of hardware that can achieve the graphs we desire for neuromorphic computing. One can envision a network of copper wires connecting CMOS neurons and synapses, but this would not function due to the capacitance arguments presented above. CMOS neurons are invariably connected by a shared switching network, and as we have argued, this leads to connectivity/speed trade offs that limit performance long before the scale that concerns us here. Superconducting interconnects do not fare better. Connecting many spiking neurons with superconducting interconnects requires active transmission lines that negate any power benefits and lead to formidable wiring and current biasing challenges. 

Optics may be promising to overcome these limitations, but in free space, routing is difficult, particularly with high fan-out and recurrent network structures. Fiber optics are far too bulky to be the only means by which neurons are connected locally, but on-chip waveguides are appealing. They are entirely passive, do not experience charge-based parasitics, and may enable neurons to have independent, dedicated axonal connections to each synapse. The size of the waveguides is limited by the wavelength of light to about 1\,\textmu m in the two transverse directions, but this is does not make the interconnection network unworkably large, provided multiple planes of compact dielectric waveguides can be employed. Frequency multiplexing may lead to effective reduction in spatial consumption by a factor of 10 or so, but such an approach puts an extra burden on light sources as well as spectral filters which generally need to be tuned. Using frequency as the only means of uniquely identifying each neuron greatly limits connectivity, but using frequency multiplexing in addition to a large number of independently routed waveguides may reduce network size, provided the source and filter challenges can be met. If these challenges cannot be overcome, monochromatic neurons each with an independent axonal tree still appear promising. Yes, they are large, particularly compared to biological neurons. But as we have argued, it is a neuron's area divided by the speed of signaling that affects network performance. With this particular connection network, signals can propagate at the speed of light across entire wafers, in free space between wafers, and over fiber to distant regions of the network. This is the reasoning that leads us to consider a photonic connection infrastructure most promising for establishing the recurrent, modular, hierarchical, small-world graphs necessary for a cognitive  architecture. 