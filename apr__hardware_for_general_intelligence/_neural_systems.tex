\section{\label{sec:neuroscience}Information Processing in Neural Systems}
Information processing in a Turing machine is a serial operation. The state of the machine is updated in a sequential manner based on instructions and the contents of memory. Neural computation departs markedly from this approach. Enormous numbers of operations in the brain occur simultaneously, with many neurons receiving communications from many neighbors and independently accessing local synaptic memory. The essence of neural information is to share the burden of computation across a large network of processors, while connecting them and signaling in such a way to enable the information of the disparate elements to be efficiently integrated in system-wide operations. Where a bit in a digital system enables one to answer a single binary question, the large number of synapses input to a neuron enable one to answer a large number of simple analog questions. Whereas the von Neumann architecture requires external memory to be read and written at each computational step, processing and memory are not distinct in neural systems. Whereas digital computing with a Turing apparatus can integrate information from separate calculations in a serial manner, neural information is, by its nature, integrated across a spatial and temporal hierarchy.

Here I review the concepts of neural information processing. One central concept is the ability for systems to achieve differentiated local processing combined with information integration across space and time. Each neuron is receptive to a certain subset of information, and it will pulse in response to presentation of that information, while remaining quiescent under other stimuli. The response of the neurons in a network is differentiated, so they each express different signals, yet a simultaneous interpretation of a broad array of information can be gained through the network as a whole. A broad range of information can be simultaneously received and processed due to the manner in which neural systems efficiently move information across space and time. Here I summarize the spatial properties of networks that enable this modality of information processing, and then discuss temporal considerations from the device to systems levels. The final subsection within this section summarizes concepts pertinent to system-wide information processing that enables cognition.

\vspace{3em}
I go into some detail regarding neural systems because these concepts are central to the design of high-performance neural systems. With some exceptions, efforts related to hardware for neuromorphic computing do not incorporate the architectural principles of neural systems, choosing instead to focus on feed-forward graphs for deep learning. Similarly, many efforts ignore the important computations occurring at synapses, treating a synapse as a simple variable attenuator. And in many efforts, the information processing performed in dendrites are neglected all together, replacing the complex computations of the dendritic tree with a point neuron model. All of these simplifications can be justified if one does not aim to create hardware for general intelligence. However, with high performance as the primary focus of this review, we cannot expect to glean the most crucial insights if we neglect to consider the basic principles of neural information processing. 
\begin{figure} 
    \centering{\includegraphics[width=8.6cm]{figures/neural_elephant.png}}
	\captionof{figure}{\label{fig:neural_elephant}}
\end{figure}

\input{__spatial_structure_of_neural_systems}

\input{__temporal_dynamics_of_neural_systems}

\input{__memory_in_neural_systems}

\input{__cognition}

\input{__summary_of_neural_information}