\vspace{3em}
In the 1840s, Lovelace contemplated the possibility of artificial machines that could think. She concluded systems such as the Analytical Engine would never be capable of thinking. In Turing's famous paper introducing the Turing test \cite{tu1950}, he refuted her position. Turing did not argue that machines would be able to think, but rather that they would be able so faithfully mimic a thinking creature that we could not tell the difference. He begins ``Computing Machinery and Intelligence'' with the sentence, ``I propose to consider the question, `Can machines think?' '' But in that paper, he asserted that this question is ``...too meaningless to deserve discussion.'' He chose instead to focus on an imitation game wherein an interrogator is in a separate room from a man and a machine, and the interrogator must determine which of the two is human and which is artificial by asking a series of questions. If the machine can fool the interrogator, it passes the Turing test. He phrases the modified question as: ``Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme...'' a digital computer can pass the Turing test. Turing argues that digital machines certainly will have this capability, predicting that by the end of the 20th century the idea that machines can think would be commonly accepted, largely due to increased speed and memory capacity of computing devices. He refutes Lovelace's assertion that ``The Analytical Engine has no pretensions to \textit{originate} anything.'' However, Lovelace was specifically considering a specific mechanical apparatus, and had no capacity to envision how electronics based on vacuum tubes would change computing, let alone silicon microelectronics. She did not see how far hardware could evolve. Maxwell did not establish the relationship between electricity, magnetism, and light until 1861 \cite{}, superconductivity would not be discovered until 1911 \cite{on1911}, and Josephson junctions were not invented until xxxx \cite{}, and it would be into the 21st century before ``superconducting single-photon detector'' became a household term \cite{}. 

I think Lovelace was correct that a mechanical apparatus like the Analytical Engine will never be capable of original thought, and I also think Turing was correct that eventually it will be commonly accepted that machines can think, although I do not agree that it is primarily a matter of increasing memory capacity and speed of execution that will lead to AI. The serial following of instructions must be replaced by the ``efficient language'' based on concepts of the nervous system, as anticipated by von Neumann in 1949, and to make this replacement, new hardware must be conceived. Silicon transistors and memory have been and will continue to be profoundly successful at performing the operations required of a Turing machine, but a Turing machine will not achieve the highest intelligence once hardware and architectures have evolved to the asymptotic limit. Perhaps Turing and Lovelace would both update their positions if they could share our vantage point early in the 21st century. We have now witnessed the explosion of silicon technology, the seeming inevitability of Moore's Law and Dennard scaling, and a steady deepening in our knowledge of cognitive science through advances in experimental techniques and persistent investigation. Turing backed away from the question, ``Can machines think?'', but I contend that is exactly the question we should be trying to answer in the 21st century. 


\vspace{3em}
Ada Lovelace contemplated artificial intelligence, as did Alan Turing. Turing also proposed modeling computation on the workings of the brain, as did John von Neumann. It has long been our intention to distill the way we think into operations that can be performed in hardware. When the principles of cognitive science are considered alongside the practicalities of engineering, this device physicist is inclined toward a new platform for cognitive hardware. The performance gains are sufficient that we accept cryogenic operation, and we thus gain access to silicon light sources, single-photon detectors, Josephson junctions, and dissipationless storage loops.

With this hardware, we can control time constants across a broad range at the device scale, enabling each neuron to participate in a broad range of dynamical states. The dendritic tree of each neuron can perform myriad computations to decipher complex inputs. Plasticity mechanisms adapt networks to fractal structures to enable efficient transfer of information across spatial and temporal scales. Communication has no delays until the scale of very large networks, indicating that such systems could achieve network-wide oscillations sampling beyond trillions of neurons contained within the fiber-optic light cone. A neuron can send a single photon to a distant region of the network and incite a new neuronal avalanche that may change the trajectory of the network dynamical state, yet the network is stable and balanced through plasticity and inhibition.

How shall we contemplate a vast network of optoelectronic neurons spread across the surface of an asteroid, with network-wide information exchange a thousand times faster than one of our minicolumns can converge? The impacts to science, technology, and the fate of humanity are considerable indeed. The hardware and architecture sketched here may prove physically or practically intractable. But such a technology is not obviously unattainable. At this early stage, uncertainty is high, but the ramifications are so great that the subject merits further scientific inquiry.

\vspace{3em}
Did you define cognition?



We conjecture that Lovelace and Turing were both right. She was right that computing machines as they were known to her, and with the serial processing Turing proposed, really are not up to the challenge of thinking. And he was right that a machine can be capable of thought and learning like a child, but to do so, a modality of operation significantly different from the sequential instruction execution of the Turing machine must be employed.


\vspace{3em}
Working in the field of beyond-CMOS computing hardware, one quickly absorbs the mantra: never underestimate CMOS. Working the the field of hardware for AI, one quickly absorbs the wisdom: never underestimate the brain. We recognize the audacity in proposing hardware to outperform CMOS for any task. Yet we think the arguments presented here make the case that it is worth pursuing silicon-based technology with superconductors, light sources, and waveguides instead of transistors and electrical interconnects for cognitive neural systems. Does this mean we are confident such hardware will lead to beyond-human intelligence? Not at all. We understand CMOS, and we know what its limitations are likely to be. But the brain maintains important secrets, even after hundreds of years of inquiry. We have laid out an architecture that achieves fractal scaling over many orders of magnitude, and appears promising for enabling communication across the hierarchy at speed far greater than biological systems. And we have tried to respect the complexity of synaptic, dendritic, and neuronal functionalities in our circuit concepts. But it is possible that the subtleties of neuronal devices and architectures are more clever than we presently comprehend, and the structures we have discussed\textemdash from circuits to systems\textemdash will not achieve the nuanced information processing that leads to advanced cognition. For example, at the device level, synapses communicate with many neurotransmitters that can be modulated independently and affect information processing differently. At the architecture level, the thalamus coordinates information processing and enables access to the global neuronal workspace in a masterful manner that unifies the signals from many brain regions into a coherent cognitive moment. It is not clear that the circuits presented here will achieve comparable complexity, and it is not clear that we will soon understand how, with optoelectronic systems, to implement something like the thalamocortical complex that integrates information across the entire network architecture. It is our perspective that progress beyond the present state requires a significant experimental effort. Hardware must be devised, and networks must be observed. Only then will we find the limits of what can be made and how well it can process information.

\vspace{4em}
Misc. notes:

\vspace{4em}
\begin{itemize}
\item origins of modern computing intertwined with WWII
\item Turing: interests, universal computation, computability, Turing machine, serial, cryptography
\item von Neumann: interests, universal computation, numerical investigation of numerous physical problems, numerically solving differential equations, digital computing, memory storing data and instructions, von Neumann bottleneck
\item Turing: ``...all digital computers are in a sense equivalent.'' \cite{tu1950}
\item cryptography leads to creation of Turing machines one side of the Atlantic, numerical analysis of nuclear weapons leads to creation of Turing machines on the other (Dyson, pg 257)
\item Shannon: communication, information in data streams, again focus is on serial information processing
\item computing hardware: vacuum tubes, punched cards lead to silicon microelectronics, si uniquely suited to accomplishing digital computing, von Neumann architecture still going strong in si
\item communication hardware: ethernet for pretty big networks, fiber-optic cables replacing telegraphs under the atlantic
\item silicon photonics is where these two meet: light for commmunication, electronics for computation, maintaining the von Neumann architecture, WDM across the von Neumann bottleneck
\item Turing's discussion of ingenuity and intuition (Dyson pg. 252): digital all ingenuity, brute force search; neuro brings intuition back and honors its role; populations of neurons enable intuition to be based on Bayesian inference rather than random guesses.
\item Turing says ``ingenuity replaced by patience''. This is very much what happens in digital neuro. Brute search takes to long to enable neural information processing.
\item computing, communication theory, and cryptography all advanced significantly during and in response to WWII. The 80 years from 1938 to 2018 have seen the emergence of transformative technologies in these fields. Much contemporary work follows in these veins. For example, the goals of a universal quantum computer are very similar to Turing's universal calculator, with the addition of quantum physics to dramatically increase the speed of certain algorithms. Because quantum states are fragile and subject to decoherence, quantum systems strike us as very poorly suited to perform the serial operations of a Turing processor requiring writing and reading to reliable memory. Nevertheless, the requirement for cryptography in a world where trust is unfounded is sufficient motivation for many to pursue quantum computers, if for no other reason than to perform Schor's algorithm.
\item Grover's search algorithm is another motivator, again following Turing's line of reasoning to replace intuition with ingenuity, and ingenuity with brute search. The problem is the physics and hardware we have at our disposal make it very difficult to realize machines capable of performing these operations efficiently. 
\item In addition to limitations resulting from the fact that it is hard to implement quantum computers in hardware, some problems simply do not map well onto Turing machines, no matter the machine's complexity. Embracing the duality of ingenuity and intuition, as a neural system does, is increasingly useful for solving many of our present problems, including those of national security and defense, and extending into realms of medicine and science.
\item ``The paradox...to understand.'' \cite{dy2012} pg. 263.
\end{itemize}

\vspace{3em}
To put the present discussion of cognitive neural systems in context, we must revisit the origin of computing. Nearly all modern computing is based on digital (binary) information processed in a von Neumann architecture, which was devised as a means to realize a Turing machine in electronic hardware.

\vspace{3em}
Turing's 1936 paper, indisputably a record of historical brilliance, has been so impactful in part due to the simplicity of the concept. It is tractable to contemplate the potential operations of a single read/write head following one-dimensional instructions. Describing this model, Turing was able to produce formal proofs about the universality of the apparatus. Yet questions of efficiency remained. 

It is far less tractable to mathematically model the capabilities of a system of a hundred billion interconnected dynamical nodes in a network of high topological dimension, as we find in the brain. It would not have been sensible to pursue neuromorphic computing systems until the limits of the Turing machine were reached, particularly considering Turing proved his machine could determine the result of any computable function.

By following the evolutionary history of the concept of a Turing machine followed by the implementation with the von Neumann architecture, it is natural to pursue neural systems with many processors following the instructions to compute the results of the equations used to model neurons. The downside of this approach to numerical emulation of neural systems is the inefficiency relative to the performance achieved by hardware embodying neural operations based on the physics of the constituent devices.

\vspace{2em}
Silicon photonics provides three primary dielectric materials that can be used for these passive waveguides: Si, SiN, and SiO$_2$. The indices of refraction of these materials are 3.5, 2.0, and 1.5, respectively, for $\lambda$ close to 1550\,nm. These are the three primary dielectrics used in CMOS technology as well. 

\vspace{2em}
At different times, a neuron firing is known by other neurons to mean different things.

\vspace{3em}
electronics has had a simple roadmap: make is smaller. this is no longer adequate, and new methods of information procession and architectures are required. AI poised to permeate every industry, 3 trillion dollar market

\vspace{3em}
Dynamical pattern of a given clulster determined by the specifics of its graph structure and device time constants

\vspace{3em}
address Turing's comment that one cannot know if a machine is thinking

\vspace{3em}
metrological advantage: you can see which neurons fire by looking at them with a camera. compare to the difficulty of obtaining high-speed data from many neurons in vivo.

\vspace{3em}
Whatever avalanche just occurred, it is always slightly less probably to have a slightly larger avalanche, and slightly more probably to have a slightly smaller avalanche. This scaling is limited on teh small side by the fact that no avalanche can have fewer than one neuron involved and on the large side by the size of the network.

\vspace{3em}
membrane time constant dynamic with current

\vspace{3em}
role of oscillations in STDP

\vspace{3em}
Reasons to publish (addressing concerns of Bostrom)

1. New/early/infancy; significant development required, both concept and hardware
2. If soens ever did prove feasible, crossover would be Bostrom's slow category
3. Will require concerted effort, probably at least 100s of people, money, foundry
4. A specific hardware proposal has the potential to offer a useful case study, perhaps leading to preparedness
5. I am an employee of the Federal Government in service of the US taxpayers, and I have an obligation to publish my research.
6. If a superintelligence powerful enough to rapidly overcome us decides in fact to do so, it may have good reason to do so
7. I strongly doubt if a fast takeover transpires. That would
8. If such a technology is a threat, the sooner we are aware of it's potentiality the better

Reasons to expect soens to be slow in achieving superintelligence (at least decades for superintelligence; more rapid, interesting progress on smaller scale on time scale of year or so)

1. Need new hardware just to determine if these circuit concepts will perform well and scale, at least 10 years for maturity
2. Need device and architecture improvements, theory and experimental capabilities, breakthroughs in understanding how to use such systems
3. Expensive, at least \$1B for human-brain scale
4. Progress will come in distinct hardware generations. We can ensure we don't produce the next iteration until we are ready
5. It will take a movement of historical proportions to realize beyond-human intelligence with soens, there is no risk of stumbling abruptly across the finish line

\vspace{3em}
other concepts to address:
\begin{itemize}
\item neuromodulators
\item gap junctions
\end{itemize}

\vspace{3em}
define deep learning in the sense that Hinton originally intended. Liquid-state machines, LSTM are not deep learning, and do make use of time domain
\cite{lebe2015,gobe2016}

\vspace{3em}
introduce neural elephant and neuromorphic elephant, figures for each

\vspace{3em}
key themes:
\begin{itemize}
\item neural information processing shows remarkable and powerful nuance from the devices to coding strategies to the architecture
\item the evolution of hardware has been driven to avoid nuance all together (binary)
\item attempting to simulate nuance by stepping through differential equations with a digital machine is inefficient
\item hardware must evolve significantly to exploit similar principles of information processing
\item superconducting optoelectronic hardware appears uniquely capable of the device complexity, communication infrastructure, and scalable architecture
\end{itemize}

\vspace{3em}
Folks to contact:
\begin{itemize} 
\item Likharev
\item Furber
\item Modha
\item Srinivasa
\item Davies
\item Segall
\item Tait
\item Prucnal
\item Harris
\item Miller
\item Aimone
\item Kadin
\item Van Duzer
\end{itemize}
	
\vspace{3em}
waves of popularity: neural networks, superconducting electronics, optical computing	
	
\vspace{3em}
Universality applies to Turing machines, neural nets (Siegelman and Sontag \cite{siso1991}), and dynamical systems (Maass \cite{}, Dambre et al., \cite{dave2012})	

\vspace{3em}
Our goal is not to draw from this literature review an exact blueprint of a thinking machine, but rather to attempt a compilation of general principles from neuroscience, dynamical systems, and computing to inform foundational decisions regarding the physics that should be built in to hardware for artificial intelligence.
	
\newpage
\appendix

\section{\label{sec:superconducting_interconnects}Superconducting Interconnects}
It is not explicitly necessary that the output of each neuron be able to switch a junction at a downstream synapse, but it is advantageous from an information processing viewpoint. Otherwise, interactions between synaptic events are limited to the few-picosecond duration of an SFQ pulse, and this restricts a neuron's ability to process information across a broad range of time scales. To get a feel for possible fan-out capacity, let us assume neuron's output JJ must switch a JJ at each receiving synapse and that 10\,\textmu A is required to do so. Consider two scenarios. First, the current from the neuron is split evenly by a passive, superconducting tree. A typical junction used for digital logic will produce 100\,\textmu A upon switching, meaning the inductance of the tree must stay below 10\,pH. Typical niobium wires have 500\,fH per square and must be approximately 1\,\textmu m wide to carry this current, so typical distances from a neuron to its synapses must be about 20\,\textmu m.


\vspace{3em}
proof of Turing equivalence of recurrent neural nets \cite{kisi1996} and can approximate arbitrary finite state automata \cite{omgi1994}. These statements taken from \cite{vesc2007}

\vspace{3em}
our synapse designs can be perfect in the sense defined on pg 42 of \cite{li1997} if a single DR loop is employed for single-spike filtering

\vspace{3em}
Between neurons in the brain, ``connections are hard-wired in the sense that each connection is made by a dedicated wire, the axon, so that, unlike processors in a computer network, there is no competition for communication bandwidth.'' \cite{ko1997}

\vspace{3em}
``...synapses continually adapt to their input, only signalling relative chages, which means that the system can respond in a highly sensitive manner to a constantly and widely varying external an dinternal environmentt. This is entirely different from digital computers that enforce a strict segregation between memory [] and computation. Indeed, they are carfully designed to avoid adaptation and other usage-dependent effects from occurring.'' \cite{ko1997}

\vspace{3em}
``...current thinking about computation in the nervous system has the brain as a hybrid computer. Individual nerve cells convert the incoming streams of digital pulses into spatiall distributed variables []. This transformation involves highly dynamic synapses that adapt to their input. Information is then processed in the analog domain, using a number of linear and nonlinear operations [] implemented in the dendritic cable structure....'' \cite{ko1997} In the brain, memory resides in many locations: synapses, dendrites, the cell membrane, and in the pattern of dynamical activity. 

\vspace{3em}
As Cristof Koch wrote in 1997 regarding biological neural systems, ``...we are left with a feeling of awe for the amazing complexity of nature. Loops within loops across many spatial and temporal scales.'' \cite{ko1997} 

\vspace{3em}
jj threshold plays the role of voltage-gated ion channels in some cases. this is essentially what Segall has described \cite{crsc2010}.

\vspace{3em}
memory: 
\begin{itemize}
\item differentiate between memory stored in dynamical state (generally relatively short term, LSTM, reservoirs); and plasticity mechanisms (STDP, metaplasticity, dendritic plasticity) that adapt the network; and short-term plasticity, which acts at the shortest time scale and filters input trains
\item none of these is the same as the concept of memory in digital computing where you can set down a bit and go back to pick it up again later
\end{itemize}

\vspace{3em}
Hodgkin-Huxley nobel prize

\vspace{3em}
back-prop in the brain:
\begin{itemize}
\item top-down may be providing error signal
\item the brain anticipates/models reality and compares sensory data to internal model. perhaps some form of back prop resides therein
\item \cite{scbe2017}
\end{itemize}

\vspace{3em}
recurrent vs feed-forward, context of visual system where both are employed
\begin{itemize}
\item feed-forward in early visual system like a CNN
\item projects to high-dimensional space
\item that high-dimensional representation projects onto a highly-recurrent network
\item Edelman's concept
\item cite Swanson, others Re visual system
\end{itemize}

\vspace{3em}
As early as 1891 Ram\'{o}n y Cajal understood that dendrites are a neuron's input devices, and axons carry the output \cite{hoko2006}.

\vspace{3em}
First layer of hierarchy is computation and communication within each neuron. Second layer is between neurons, and only here do we introduce optical communication. This is accomplished with silicon light sources. Perhaps there is a scale at which more efficient light sources are required, for example, by large hub nodes serving millions of connections. For these, perhaps III-V sources can be utilized. If so, I suspect they will be implemented primarily in specialized modules. Certain brain regions, perhaps comprising tens of thousands of wafers, will utilize silicon light sources for intra-modular communication. Globally influential modules, like the thalamus, could be manufactured in a differe, more expensive process with brighter light sources. As long as only one such module is needed per $x$ local modules, and the price of manufacturing the modules with III-V sources is $x$ more expensive, it may be economically and technologically feasible. As long as both types of modules transmit and receive input to and from standard optical fibers, they will be able to communicate. Such considerations only become pertinent beyond the extremely large neuromorphic scale, which we define loosely as far beyond human intelligence. 

\vspace{3em}
To support or refute the SOEN hypothesis:
\begin{enumerate}
\item efficiency of Si light sources
\item feasibility of massive multiplanar
\item JJ yield
\item more thorough area analysis of waveguides and mutual inductors
\item fiber optic volume studies in full architectures
\item eoeoe
\item noise considerations
\end{enumerate}

\vspace{3em}
 \cite{bosp2015} ``Tractography deals only with white-matter organization, not the cellular origin and synaptic termination of connections in gray matter.''

\vspace{3em}
the primate brain has a conscious bottleneck and can only consciously access a single item \cite{dela2017}. should we expect this limitation to hold in artificial hardware as well?

\vspace{3em}
``The risk, on the one hand, is of forgetting that one has oversimplified the problem, one may forget or even deny those those inconvenient facts that one's theory does not subsume.'' \cite{he1949} pg. xiii

\vspace{3em}
``One can discover the properties of its various parts more or less in isolation, but it is a truism by now that the part may have properties that are not evident in isolation, and these are to be discovered only by study of the whole intact brain.'' \cite{he1949} pg. xv