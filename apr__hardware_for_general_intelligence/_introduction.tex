\section{\label{sec:introduction}Introduction}
The relationship between the physical substrate of the brain and the information processing occurring therein has long been and remains among the most significant scientific subjects. From a philosophical perspective, we would like to know whether systems other than biological brains devised through natural selection can give rise to intelligence similar to our own. From a computational perspective, we would like to understand the means by which the brain maps complex and evolving stimulus into a coherent context. From a physical perspective, we would like to know which elements and properties of the universe can be combined to enable the computations of cognition. And from a technological perspective, we would like to know if the candidate systems can be feasibly and reliably produced as well as the magnitude of the scientific and economic impact.

To begin, let us define important terms. (define cognition, consciousness, and general intelligence)
General intelligence is the ability to assimilate knowledge across content categories and to use that information to form a coherent representation of the world.

In this review I consider the requirements placed on hardware if it is to achieve information processing in the model of and at the scale of the human brain. The last 30 years have brought tremendous gains in both computing and neuroscience, and as a result we are witnessing a flourishing of brain-inspired computing. Here I draw from the domains of digital computing and device physics as well as the cognitive sciences in an attempt to identify guiding principles to enable the realization of artificial hardware capable of intelligence and broadly useful as a scientific and mainstream technology.

We begin in Sec.\,\ref{sec:history} by reviewing the historical developments that led to the present context. Important concepts from neuroscience are summarized in Sec.\,\ref{sec:neuroscience}, and we use these principles to inform our design of hardware. Section \ref{sec:electronics} describes neural systems based on CMOS microelectronics and discusses motivations for pursuing alternative devices and physics for large-scale neural systems. Among alternatives, I focus in this article on photonic technologies (Sec.\,\ref{sec:photonic_neural_systems}) and superconducting electronic technologies (Sec.\,\ref{sec:superconductors}). The main thesis of this work is that the combination of integrated photonic and superconducting electronic devices will be conducive to the realization of large-scale cognitive systems. Specifically, in Sec.\,\ref{sec:superconducting_optoelectronic} I describe superconducting optoelectronic hardware utilizing photonic communication between neurons that compute with superconducting electronic circuits. Superconducting single-photon detectors enable communication with as few as one quantum of the electromagnetic field, and wafer-scale networks of integrated-photonic waveguides route photonic pulses from each neuron to its thousands of synaptic circuits. Superconducting circuits enable dissipationless memory, and Josephson junctions provide naturally neuromorphic thresholding and spiking behavior. Scaling considerations are presented in Sec.\,\ref{sec:scaling}, and unique application spaces are considered in Sec.\,\ref{sec:applications}. Ramifications of such technology are discussed in Sec.\,\ref{sec:outlook}.

%Optoelectronic neural systems reside at the confluence of multiple disciplines. The subject is derived in large part from the foundations of communication theory and computation, as established by Turing \cite{tu1936}, von Neumann \cite{ne1945}, Shannon \cite{sh1948} and others. Yet the nature of information processing in neural systems is not wholly captured by the mathematical analysis formalized to describe serial communication and computation with digital signals. Concepts from neuroscience dating back to Ramon y Cajal \cite{}, Mountcastle \cite{}, Edeleman \cite{}, and many others during the flourishing of the last 40 years indicate that clean, serial data streams have much to gain from the network concepts of neural systems with complex spatial and temporal behavior, particularly if one seeks a machine that can think like an intelligent being. If one seeks complexity of performance, one must provide complexity in hardware. If one seeks comprehension in the face of ambiguity, the computation must be able to handle shades of gray. Probability and statistics are inherent to neural systems. 

%Conceptually, neural systems contain threads of communication theory, digital logic, probability and statistics. In practice, these systems combine the physics of many devices. The systems envisioned here utilize electrons to compute and photons to communicate. They leverage semiconductors to make light and superconductors to compute. At this confluence of physics and computing, it is possible to conceive of systems with complexity from the chip scale to the globe, employing the logic of neural systems for cognitive information processing across broad reaches of space and time. 

%Why do this now? Why this way? Since the inception of the EDVAC, its limitations were anticipated \cite{}. Yet the microelectronic march delayed the consequences by many decades. The scaling first charted by Moore \cite{} left little impetus for revolutionary concepts. Now that scaling is reaching physical limitations \cite{}, there is an appetite for what new hardware may have to offer. And as computing machines have become deeply integrated in society, we are ready for a sea change in what we ask our machines to do. They can answer any question of fact, but that is no longer all we ask of them. We seek intelligent machines, computers that think, not superficially, but with as deep a wisdom as we can manage to construct. That is why we seek to combine the strengths of superconductors and light. In conjunction, we think, they will lead to the most intelligent machines. To put this discussion in context, we must revisit the origins of computing.

\input{__historical_context}

\vspace{3em}
Why is neural computing difficult? Largely it is because the problems to be solved are not clearly delineated. Whereas the problems digital computers were invented to solve have clear inputs and a well-defined algorithm for computing a solution, the utility of neural systems resides in their ability to deal with disparate inputs and identify salient information even when the objective of the computation is only vaguely specified. ``Computational problems that lend themselves to algorithmic solutions share a characteristic property: they are structured, meaning they can be stated clearly and concisely in mathematical terms.'' \cite{abps1987} ``Problems such as pattern recognition in natural environments, however, lack the structure that would allow simple algorithmic solutions.'' \cite{abps1987} Solution of these types of problems requires learning correlations among the elements of large data sets based on repeated observations of relevant elements of the data. ``A computer [ ] cannot draw on this reservoir of common experience; everything must be spelled out for it precisely and unambiguously. ...there is a major component of irregularity that does not fit any simple mathematical or algorithmic model.'' \cite{abps1987}

After learning many relevant correlations through observation over time, a neural system is able to quickly identify salient features of a stimulus in a given context, not by serially searching through vast memory stores, but through associative memory, wherein ``partial features of an object trigger the retrieval of complete information about the object.'' \cite{abps1987} Such associative memory and recall has played a major role in shaping machine learning, particularly in models such as Hopfield networks \cite{ho1982} and reservoir computers \cite{}, both of which are inspired by highly connected, recurrent graphs such as that found in the hippocampus, which plays a central role in learning and memory in biological organisms \cite{}.

\vspace{3em}
As Prucnal and Shastri state, ``engineering quote''. The goal of a physical consideration is complimentary to this, intending to answer the questions: What are the physical limits to cognitive hardware? Which devices and physical mechanisms allow us to maximize information integration across space and time? This article considers these questions of physics and information, but from a pragmatic perspective regarding manufacturability. One may imagine a system of arbitrary complexity with devices achieving extreme accuracy in all parameters, but if such a system cannot be realized due to practical limitations, it is not a useful response to the questions above, and we cannot expect it to become a mature technology. The present article is focused on ideas regarding technology incorporating appropriate physical mechanisms for neuronal computation and communication while heeding the insights of very-large-scale integration regarding scalable manufacturing and device performance margins. 

\vspace{3em}
A primary assumption of my perspective is that communication in neural systems is best achieved with telecom photons, starting at the scale of communication between neurons and extending across very large, multi module systems. It stands to reason that the highest system energy efficiency will be gained if communication uses the fewest number of photons possible, limited by noise. This perspective regarding few-photon communication sets the course for many of the decisions made regarding hardware.