\subsection{\label{sec:photonic_neural_systems}Photonic Neural Systems}
Like AI and superconductors, there have been several waves of photonics in computing. One theme is the use of light for highly parallel computation, exemplified by the special case of the Fourier transform. The late 1980s brought a surge in optical computing, with some efforts intending to displace silicon electronics for digital logic, such as with bistable lasers, and sometimes for the purpose of augmenting silicon electronics. Displacing Si electronics has failed for the reasons Keyes has pointed out.

\vspace{3em}
Historically, optics in computation has aspired to achieve special-purpose accelerators, such as Fourier transform devices (which can be implemented in free space with a lens, but also in slab-guided modes in integrated photonic systems \cite{shha1968,anbo1977}), as well as all-optical digital computing systems. At present, a major goal of photonics is to augment CMOS electronic hardware to aid in communication. Optical communication shows indisputable advantages over long distances, as exemplified in global fiber optic networks as well as local-area networks. On the chip scale, the advantages of optical communication must contend with the challenges of optoelectronic hardware integration. 

\vspace{3em}
Foundations of optical waveguide theory can be found in Refs.\,\cite{snlo1983} and \cite{hu2009}.

\subsubsection{Optics for Digital Computing}
%integrated-optical approaches to numerical optical processing
\cite{ve1984} In 1984, Verber stated, ``A problem which has been common to almost all efforts to perform numerical computations by optical techniques has been the accuracy limitation imposed by the inrinsically analog nature of the devices. Although the ultimate solution to this problem is generally accepted to be in the application of optical bistable devices to produce fully binary optical systems, the long development times anticipated for these systems has led to several other approaches to higher accuracy computation using analog optical techniques.'' Approaches to improving the accuracy of analog optical computations have been proposed \cite{psca1980,arha1984}, but neither analog In 2019, these problems have not been solved, and binary optical systems have not proven capable of displacing CMOS for digital computing. 

\vspace{3em}
After decades of research, the phrase ``all-optical'' now sets off alarm bells, and one must ask if the advantage gained (usually speed) is worth neglecting to utilize the myriad competencies of electrical circuits.

\vspace{3em}
This one demonstrated a functional photonic logic device
%Symmetric Self-Electrooptic Effect Device: Optical Set-Reset Latch, Differential Logic Gate, and Differential Modulator/Detector
\cite{lehi1989}

And this one used it in a system
%Six-stage digital free-space optical switching network using symmetric self-electro-optic-effect devices
\cite{mccl1993}


\vspace{3em}
``...it is the need to limit power dissipation that largely constrains clock rates in current electronic devices\textemdash lower operating voltages give slower speeds but correspondingly lower energies per operation.'' \cite{mi2010}.

\vspace{3em}
``...we have to remind ourselves that the field of integrated optics is still in its infancy, still in its research stage, and still searching for its proper role.'' \cite{ko1981}
``The arguments for optical wiring are understood even down to the chip-level, but chip-scale optical interconnect technology is still in its infancy.'' \cite{mi2010}

\subsubsection{Free-Space Optical Neural Nets}
The late 1980s 


\vspace{3em}
holographic gratings in photorefractive crystals store associative memories and use backprop for training, similar to recent Shanhui Fan \cite{waps1987} nonlinear etalon for sigmoid response; intended for image classification, not much different than deep learning today with sigmoid activation functions and backprop. ``This architecture combines the robustness of the distributed neural computation and the backpropagation learning procedure with the hight speed processing of nonlinear etalons, the self-aligning ability of phase conjugate mirrors, and the massive storage capacity of volume holograms to produce a powerful and flexible optical processor.'' Potential reasons this technology didn't catch on: free space optics, bulky, difficult to package, not fieldable, requires experience with optics to operate, difficult to scale to large systems, nonlinearity utilizes bistability \cite{gi1985} which has a history of being difficult to manage, based on photorefractive effect which is volatile, free-space interconnectivity is bulky and difficult, requires many sources, high power for nonlinear effects for hologram and etalon, how to get data in?, fan-out limited by gain of nonlinear devices and dictates an information-collapsing network architecture

\vspace{3em}
\cite{psbr1988}
``...in an optical implementation each grating corresponds to a separate interconnection between two neurons...''

\vspace{3em}
``It is the ability to establish an extensive communication network among processing elements that primarily distinguishes optical technology from semiconductor technology in its application to computation.'' \cite{abps1987} However, in that and other related work at the time, the objective was to utilize optical signals propagating in free space to interconnect neural processing elements. Using light in this manner has the advantage that ``multiple beams of light can pass through lenses or prisms and still remain separate,'' \cite{abps1987} yet routing of free-space optical signals brings new challenges for complex neural systems. To achieve connectivity graphs corresponding to neural networks with feed-forward, feed-back, and recurrent connections, light cannot travel only in straight lines, but rather must branch and change direction many times. Construction of complex networks with free-space optics, mirrors, and lenses quickly leads to issues related to scaling. 

As Goodman pointed out in 1985, ``quote from Goodman's paper'' The benefits of light for fan-out can be difficult to harness in free space. We may expect the situation to improve with on-chip waveguides. ``Two beams of light, unlike a pair of current-carrying wires, can cross without affecting each other.'' \cite{abps1987} At the same time that free-space optical neural computers were being developed based on holographic memory, the field of integrated photonics was emerging.

\vspace{3em}
Switching elements made from nonlinear optical materials require too much power to scale, and often require III-V materials. This leads to discussion of optical transistors, in which one optical beam controls the transmission of another. ``Each element can be either a purely optical switch or an optoelectronic combination of light detector, electronic switch and light emitter.'' \cite{abps1987}
 
\subsubsection{Free-Space Reservoir Computing}

\subsubsection{Photonic Delay Systems}
In delay systems time is used to emulate space. This makes efficient use of hardware in that very few nodes can behave as many. This delay technique is a form of time multiplexing, and as such scaling results in significant latency. Such systems are useful in situations where power and hardware resources are at a premium, but this use of space and time is not conducive to the fractal use of space and time associated with cognition.

\subsubsection{Photonic Fiber Neurons}
 
\subsubsection{Integrated Photonics and Superchips}
Soref and Bennett and active silicon photonic devices in the late 1980s

\vspace{3em}
Goal was to follow the model of the integrated circuit, utilize lithographic fabrication and system complexity that can be achieved through integration of components on a chip. 

\paragraph{Components Required for Integrated Photonics}
\begin{itemize}
\item passives: waveguides and routing, beam splitters and power taps (y-junctions, evanescent couplers, adiabatic 50-50 beam splitters), spectral filters (microrings \cite{ra2007} and gratings), waveguide crossings
\item sources
\item detectors
\item electrooptic: modulators and phase shifters (rings, MZIs \cite{ohno1975}), SAW transducers for imparting a phase shift
\end{itemize}

\paragraph{Materials for Integrated Photonics}
\begin{itemize}
\item III-V
\item LiNbO$_3$
\item silica
\item silicon
\item SiN
\end{itemize}

\vspace{3em}
Active integrated photonic components were beginning to be developed in the mid 1970s \cite{ohno1975}, and by the mid 1980s many elements of the field of integrated photonics were taking shape \cite{ve1984}. Primary applications included RF signal processing and analog numerical processing \cite{ve1984}. Proposals for all-optical digital computers were made based on the key element of a bistable optical cavity. However, the important sub-field of silicon photonics had not yet emerged.

\vspace{3em}
In 1987, Soref and Bennett introduced the concept of using the shift in index of refraction that results from free carriers in silicon to achieve active optical components based on silicon waveguides \cite{sobe1987}. This insight would have to wait until the development of silicon-on-insulator wafers in the early 2000s to be put into practice. Since then, an explosion of activity has occurred in the rapidly developing field of silicon photonics. Electro-optic effects have been used to make a variety of modulators \cite{rema2010} operating into the 10s of GHz based most commonly on Mach-Zehnder interferometers \cite{lisa2005} or microring resonators \cite{xuma2007}. In addition to the free-carrier electro-optic effects, in 1993 Soref also pointed to thermo-optic effects as a means to make dynamic photonic components on an optoelectronic chip \cite{so1993}. The combination of electro-optic effects for fast index perturbation and thermo-optic effects for slow resonance tuning, in conjunction with etched silicon waveguide structures in silicon-on-insulator substrates, established a foundation of active components capable of signal switching, filtering, and modulation. In his 1993 paper, titled \textit{Silicon-based optoelectronics}, Soref presented a more expansive view of the potential for what he termed ``superchips'' that combine the strengths of photonics and electronics monolithically on a single silicon chip. Silicon had long been the material of choice for integrated microelectronics, but Soref had identified a path to make silicon also a powerhouse in photonics as well.

To make use of silicon as a waveguiding medium so that the active components described above can be implemented, one must utilize light with photon energy less than the band gap of silicon ($E_{\mathrm{g}}=1.17$\,eV/$\lambda = 1.06$\,\textmu m at 0\,K; $E_{\mathrm{g}}=1.11$\,eV/$\lambda = 1.12$\,\textmu m at 300\,K). The buried oxide of silicon-on-insulator wafers becomes absorptive for $\lambda \gtrsim 2$\,\textmu m. Thus, the transparency window of silicon-on-insulator waveguides enables operation with wavelengths below 1.2\,\textmu m, and includes the important telecom bands (O-band:1260\,nm-1360\,nm; C-band: 1530\,nm-1565\,nm), whose significance results from the very low attenuation of optical fibers at these wavelengths. Thus, silicon integrated photonic components can be interfaced with optical fibers for communication across long distances. 

Yet if a material is transparent, it is not efficient for detecting light. To create photodetectors in silicon waveguides, two approaches are taken. One approach is to utilize SiGe regions patterned in Si waveguides, as the band gap of Si is narrowed by the incorporation of Ge. Germanium is present in many contemporary CMOS processes for strain engineering, and can be economically incorporated in the foundry because, like silicon, it is a group IV element, and therefore shares process compatibility and does not act as a dopant in Si. Waveguide-integrated \cite{} and resonator-integrated \cite{} SiGe detectors operating at the O-band and C-band. These detectors have been demonstrated with high efficiency approaching 1\,A/W. The other approach is to introduce defects the silicon lattice, either through ion implantation or the use of poly-crystalline or amorphous silicon. These defects introduce absorptive states within the band gap. Detectors based on this principle have been demonstrated with $x$\,A/W responsivity \cite{meor2014}. 

\subsubsection{The Introduction of Silicon-on-Insulator}
SOI in early 2000s, guiding light on a chip is a different ballgame

\vspace{3em}
Integrated Silicon Photonics for Communication above a certain length scale in digital electronics
-monolithic with processors?
-in package?
-off-chip light sources?
-WDM

\subsubsection{Deep learning with silicon photonics}
Like superconducting neural systems, the goal of nearly all efforts in optoelectronic neural systems and neuromorphic photonics is not to develop general intelligence, but rather to realize neural systems for specific tasks such as inference or control. For most efforts, the motivation for using light is the speed, either of laser cavity dynamics or optical communication. Device and hardware choices toward these ends may be different than for the focus of this article, which is general intelligence. We intend to explain why specific choices are not conducive to the present goal, even if they are suitable for other applications.

We consider deep learning to be based on feed-forward networks of non-spiking neurons trained through a supervised algorithm such as backpropagation. While markedly different from the recurrent networks of dynamical nodes that learn from experience through local plasticity mechanisms, the relative simplicity of deep learning makes it a natural place to begin utilizing principles of neural information processing. Feed-forward neural networks have been studied with free-space optics since the height of optical computing excitement in the late 1980s and early 1990s, and after the developments in silicon photonics following Soref, similar principles were developed in an integrated context. 

%plunked this paragraph down after the rest
Matrix-vector multiplication has been a draw toward optics for some time \cite{godi1978,ve1984,maar1987}, with an early proposal appearing on pg.\,1 of vol.\,2 of Optics Letters \cite{godi1978}. Other approaches to matrix-vector multiplication have emerged over the years as technology has evolved, and the approach that is currently receiving the most attention is based on the concept of implementing a unitary operator with an array of Mach-Zehnder interferometers by Reck et al. in 1994 \cite{reze1994}. With the addition of loss or gain, any matrix can be represented through its singular-value decomposition \cite{st2016}. Because additional MZIs can be used to discard light, the full singular-value decomposition and matrix-vector multiplication can be performed using MZIs. Implementation of such MZI networks with silicon photonic waveguides is currently being pursued \cite{mi2015_fix_ref,shha2017}, and it has been argued that efficient training through backpropagation can be implemented in optoelectronic integrated circuits with tunable MZIs and photodetectors working on conjunction with CMOS logic at each interferometer \cite{humi2018}.

%another out-of-context comment
hybrid devices utilizing an artificial nonlinearity implemented with an electro-optic effect and electrical feedback have been studied since the early 1980s \cite{sm1980,ko1981}

The operation of synaptic weighting in deep learning reduces to matrix-vector multiplication. Such an operation can be achieved with an array of Mach-Zehnder interferometers. A recent demonstration accomplished this using thermo-optic phase shifters with silicon waveguides \cite{shha2016}. A network with four inputs and outputs was trained to classify four vowel sounds. The effort led to two start-up companies attempting to commercialize the technology to compete with specialized CMOS processors (such as tensor processing units) for deep learning. The photonic approach demonstrated so far made use of off chip light sources and detectors, and applied the nonlinearity in software. For such an approach to be competitive, significant system integration is required. The two senior authors of Ref.\,\cite{shha2016} have more recently moved back to a free-space approachin to deep learning \cite{}.

The approach of using 2-D arrays of interferometers for routing and synaptic weighting pursued in Ref.\,\cite{shha2017} is incompatible with large-scale cognitive systems for several reasons. One reason is that the index shifts induced by thermo-optic phase shifters are small, and power dependent, leading to either large structures, high power consumption, or both. Cross talk between thermal elements necessitates placing the waveguides far apart, and it is difficult to utilize the vertical dimension interferometer arrays, so attempting to scale results in networks that are sprawling in the plane. Further, as described in Sec.\,\ref{sec:neuroscience}, an important mechanism of learning in spiking neural systems is through STDP, wherein the activity of the two neurons associated with a synapse leads to memory adaptation. With interferometer arrays, changing a single phase in the network will, in general, modify several synaptic weights. Therefore, while backpropagation can be implemented with such a network \cite{humi2018}, STDP cannot. 

\subsubsection{Integrated Photonic Reservoir Computing}

\vspace{3em}
Recurrent neural networks can approximate the trajectory of a dynamical system \cite{funa1993}
Recurrent neural networks are Turing equivalent \cite{kisi1996}

%Reservoir computing concept introduced in:
\vspace{3em}
%Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication
\cite{jaha2004}

\vspace{3em}
%Reconfigurable semiconductor laser networks based on diffractive coupling
\cite{brfi2015}

\vspace{3em}
%Reinforcement learning in a large-scale photonic recurrent neural network
\cite{buma2018}
Free-space optics; scalable to order $10^5$ nodes; demonstrated reinforcement learning with 2025 nodes; each node is a pixel of a spatial light modulator; connections established with diffractive optical element; learning with a digital micromirror device. All weights are positive and all output weights are binary.

The general architecture is not conducive to very large scale. The elements that perform the computation are based on integration of digital electronics with mechanical mirrors. The routing is not conducive to general networks with hierarchical construction and Rentian scaling. It is hard to get light out of the module. The power per node is high because light levels are high (compared to single photons). Optical setup sensitive to alignment. Requires external control for learning procedure. Readout of network state performed with a camera. The optoelectronic system steps through discrete time with a specified update equation. In the 2018 demonstration, update of 900 nodes occurred at 5\,Hz, limited by software control of the spatial light modulator. Diffractive optical element establishes synaptic weights. Scaling limit due to ``the imaging setup's field of view and not by the concept itself.''

This is not a model for spiking neural networks. Synaptic weights are static after the training phase and do not permit any of the short- or long-term plasticity mechanisms nor dendritic processing that are central to the manner in which spiking neurons utilize time. This network has binary outputs that are readout in parallel. Internal node states are represented by optical field intensities, and therefore such a concept cannot be extended to the single-photon domain. Memory reconfiguration DOE is passive, but light is always on.

``We demonstrate a network of up to 2025 diffractively coupled photonic nodes, forming a large-scale recurrent neural network.'' Terms like ``large-scale'' need to be defined in introduction (large scale means at least 1000 logic gates, tens of thousands of transistors per chip, VLSI still refers to chips with tens of billions of transistors per chip.)

\vspace{3em}
%Advances in photonic reservoir computing
\cite{vabr2017}

\vspace{3em}
%Information processing using a single dynamical node as a complex system
\cite{apso2010}

\vspace{3em}
%Optoelectronic {R}eservoir {C}omputing
\cite{padu2012}

\vspace{3em}
%All-optical reservoir computing
\cite{dusc2012}

\vspace{3em}
%Parallel photonic information processing at gigabyte per second data rates using transient states
\cite{brso2013}

\vspace{3em}
%Toward optical signal processing using {P}hotonic {R}eservoir {C}omputing
\cite{vadi2008}

\vspace{3em}
%Brain-{I}nspired {P}hotonic {S}ignal {P}rocessor for {G}enerating {P}eriodic {P}atterns and {E}mulating {C}haotic {S}ystems
\cite{anha2017}

\vspace{3em}
%Tutorial: {P}hotonic neural networks in delay systems
\cite{brpe2018}
``Inspired by a strongly simplified interpretation of the human brain's structure, large numbers of simple nonlinear elements (neurons) are connected (synaptic links) into large networks. Information processing in ANNs usually relies on numerous simple nonlinear transformations and large scale linear matrix multiplications. The implementation of these operations in von Neumann architectures is highly inefficient, as it requires massive parallelism.''

``[E]ven the recent astonishing developments cannot mask the fact that currently no ideal ANN-specific hardware, which fully implements physical hardware neurons and physical \textit{synaptic links}, exists. With such a novel platform, many orders of magnitude could be gained in speed and energy efficiency.''

``Reservoir computers offer a compromise between performance and an implementation-friendly ANN topology.''

Time-delay systems are an approach to reservoir computing that minimize the demands on hardware by extending computation across time rather than across space. A single nonlinear node can perform a computation similar to that achieved by a neural network, but the computation is spread out across time rather than space \cite{brpe2018}. The state of a node during a time window in a delay system corresponds to the state of a node at a spatial location in a neural network. Such an approach is appealing in the context of emerging hardware because a system comprising only a single node can perform a useful calculation, albeit with a slowdown due to time multiplexing proportional to the number of emulated nodes. Using the time domain in this way is somewhat reminiscent of information processing in biological neural systems wherein each node is engaged in different transient ensembles at different times, although the specific mathematical formalism describing delay systems as manifestations of recurrent neural networks are not nearly as general as the information processing principles of the brain, nor do they aspire to be.




\vspace{3em}
%All-{O}ptical {R}eservoir {C}omputing on a {P}hotonic {C}hip {U}sing {S}ilicon-{B}ased {R}ing {R}esonators
\cite{cosc2018} 

\vspace{3em}
%Delay dynamics of neuromorphic optoelectronic nanoscale resonators: {P}erspectives and applications
\cite{rofi2017}

\vspace{3em}
%Conditions for reservoir computing performance using semiconductor lasers with delayed optical feedback
\cite{bubr2017}

\vspace{3em}
%A Unified Framework for Reservoir Computing and Extreme Learning Machines based on a Single Time-delayed Neuron
\cite{orso2015}

\vspace{3em}
%Ultrafast photonic reinforcement learning based on laser chaos
\cite{nate2017}


\subsubsection{Spiking Neurons with On-Chip Excitable Lasers}
%\cite{pena2018} Neuromorphic Photonic Integrated Circuits IEEE JSTQE
While the interferometric approach to deep learning discussed above makes use of static neurons, several approaches to spiking neurons have been pursued as well. One class of spiking photonic neurons leverages the carrier dynamics in compound semiconductor laser cavities. It has long been known that the equation governing lasers with gain and saturable absorber regions are isomorphic to the leaky integrate-and-fire neuron \cite{dukr1999}, with the number of excited carriers in the laser playing the role of the membrane potential. This correspondence has led to several designs \cite{nata2013} and experimental efforts (see Ref.\,\cite{prsh2017} and reference therein) to leverage this behavior to make spiking neurons that sum optical signals and produce optical pulses when a threshold has been reached. This work began in Er-doped fibers, and continues with on-chip implementations with III-V photonic systems, with much of the work being done in the Prucnal's group at Princeton. The refractory period of such neurons is set by the cavity photon decay time and is on the order of 10\,ps, while the integration time is set by the carrier relaxation time, and is on the order of 100\,ps. This short refractory period means such neurons can fire up to $10^9$ times faster than biological neurons, yet the short integration time means temporal correlations amongst neuronal firing events is forgotten rapidly. 

While the goal of these efforts in excitable lasers is to perform neuro-inspired computing very rapidly with small networks, and not to achieve brain-scale systems, we nevertheless point out two features of this approach to using light in neural systems that are not conducive to achieving large-scale systems. The first is power consumption. To properly set the threshold of these neurons, the gain region must be continuously pumped. This requires between 100\,mW and 1\,W per neuron, even when the neuron is not firing. For a system of $10^{10}$ neurons, a gigawatt would be consumed, even with the system at rest. The second limitation regards computation. As discussed in Sec.\,\ref{sec:neuroscience}, neural information processing leverages many complex computations in synapses, dendrites, and neurons. In excitable lasers, all the computation occurs in the interaction between photons and carriers in the laser cavity. Multiply-accumulate operations can be performed with leak and threshold, but no path toward short-term synaptic plasticity or dendritic processing have been proposed. By relying on the exponential decay constants of photons and carriers, one is unable to tune the range of temporal information processing or supply the dendritic arbor with information across a wide range of temporal scales. These computations and time constants are more readily achieved in the electronic domain with circuits that can be engineered to perform complex functions rather than relying on material parameters, a point we revisit below.

\subsubsection{Wavelength-Division Multiplexing for Routing and Synaptic Weighting} 
In addition to the work on excitable lasers as spiking neurons, the Princeton group has also pioneered the use of concepts from wavelength-division multiplexing for both signal routing and synaptic weighting \cite{tana20142,tafe2017}. Within this framework, each neuron within a cluster produces or modulates light at a distinct wavelength upon firing. The signals from all neurons within the cluster are multiplexed onto a single broadcast waveguide, and all other neurons tap all colors from this waveguide and apply synaptic weights based on the frequencies of microring resonances relative to the neuron wavelengths. For a cluster of $N$ neurons, $N$ different colors of light must be generated, $N$ microring filters must be used to multiplex these signals onto the broadcast waveguide, and each neuron must have $N-1$ microring filters to receive and weight the signals from all the other neurons. Thus, a cluster of $N$ neurons requires $N^2$ microring resonators. This approach to communication between neurons is referred to as ``broadcast-and-weight'', and is closely related to the operation of wavelength-division multiplexing in fiber communication networks.

Again, the goal of the work from the Princeton group is not to achieve brain-scale systems, but rather to ``...find out the minimum ensemble of behaviors that are necessary to harness similar processing advantages.'' \cite{prsh2017} Nevertheless, adopting wavelength-division multiplexing concepts from larger-scale communication networks down to the chip scale is intuitive and aesthetically appealing, so it is worth pointing out why it ends up not being conducive to reaching large-scale cognitive systems. To begin, it is important to distinguish between using the wavelength of light for multiplexing multiple signals on a broadcast bus and the use of microring resonators to establish synaptic weights. The Princeton group uses both techniques, but it is possible to employ one or the other independently. When using wavelength for multiplexing, the advantage is that space can potentially be saved. Instead of each neuron having an independent axonal arbor to reach its downstream connections, many neurons share a single distribution waveguide. However, the area saved is significantly reduced by the fact that $N^2$ microring resonators must be employed. More important than area is power. Because microring resonances are so sensitive to minor variations in fabrication, each of the $N^2$ resonators must be actively aligned to the appropriate wavelength corresponding to the emission from the associated neuron. This typically requires on the order of 1\,mW. For a brain-scale system of $10^{14}$ synapses, 100\,GW would be required just to align the communication network. The power consumed for alignment limits scalability, but so does the procedure for carrying out the alignment. Each of the microrings must be aligned, and if thermal tuning is employed, significant cross-talk will occur. Implementing such alignment for systems of more than a few neurons becomes quite cumbersome. Additionally, the wavelengths of the neurons can only be spaced so closely if cross talk is to be avoided, and the gain bandwidth of the light sources is limited, so a limit of roughly 200 neurons within a cluster is encountered. One may think of such a cluster as analogous to a mini-column in the brain, but unfortunately communication between mini-columns is hindered by the use of wavelength for multiplexing. In order to communicate between mini-columns, a neuron must first communicate from its local cluster up to a higher level of hierarchy where the same colors are re-used, and then down again to the target cluster. Such a communication protocol severely limits the graph structures and path lengths that can be achieved (see Sec.\,\ref{sec:neuroscience}. It is intuitive to leverage wavelength multiplexing in photonic neural systems to maximize use of bandwidth, but when used in this way wherein each neuron is uniquely identified by a color, scalability is severely hindered.

These considerations pertain to using wavelength for multiplexed routing, but there are independent reasons why using microring resonators to establish synaptic weights is not conducive to scaling. One challenge associated with microring weight banks is the fact that by changing a certain parameter (power delivered to heater, for example) the synaptic weight first increases, then saturates, the decreases as the resonance passes the target wavelength. This makes it very difficult for supervised or unsupervised learning to occur. Additionally, the shape of the resonance is nonlinear with very steep sections. Thus, to achieve uniform changes in synaptic weight, a nonuniform change in drive must be applied, and across much of the range of weights, the synaptic weight will be very noisy.

Microring weight banks and Mach-Zehnder interferometer networks have two things in common: they both require implementing phase shifts in photonic components (which usually draws power, even in the steady state), and neither is capable of implementing STDP or other unsupervised learning techniques. To achieve the largest-scale neural systems, it is highly advantageous if storage of a synaptic weight draws no power. For a system at the scale of the brain, if each synapse draws even 10\,nW in the steady state, the system will consume 1\,MW just to remember what it has learned. 

\subsubsection{Phase Change Materials for Synaptic Weighting and Neural Thresholding}
One technique for establishing synaptic weights between neurons signaling with light is to leverage phase-change materials \cite{chri2017}. Such materials have the property that the coefficient of optical absorption is different between the two phases. Therefore, a variable attenuator can be devised wherein the crystallization state of a small patch of phase-change material integrated on a waveguide determines how many photons are transmitted through the synapse. Reference \cite{chri2017} showed that such a synapse could be used to implement a form of Hebbian learning, wherein two pulses incident closely in time could strengthen the synaptic weight by adjusting the crystallinity of the material and reducing absorption. 

Such Hebbian update in this system represents a novel route toward synaptic weighting in photonic neural systems. Unfortunately, the material studied in Ref.\,\cite{chri2017} requires billions of photons for Hebbian update, thereby exceeding the communication energy limit of a single photon by at least nine orders of magnitude. Additionally, the patch of phase-change material has no way of keeping track of the order in time or even the source of input pulses, so anti-Hebbian synaptic weakening cannot be achieved, and a route to full STDP has not been proposed. 

\subsubsection{Synaptic weights in the electronic domain}
We have discussed here three approaches to establishing synaptic weights in photonic neural systems: interferometric networks; microring resonators; and phase change materials. These approaches all have one thing in common: they treat the synapse as a variable attenuator, and change the weight by varying the number of photons that pass through the synapse. Communication in biological neural systems is binary, and the synaptic weight is enacted based on how much post-synaptic current is generated, and is independent of the amplitude of the action potential reaching the pre-synaptic terminal. By contrast, if one establishes the synaptic weight in the photonic domain, communication is analog, and the number of photons in the pulse\textemdash analogous to the amplitude of the action potential\textemdash now carries information. This has two detrimental consequences. First, it requires that each neuron produce more photons that would be necessary for binary communication, and many photons are discarded at weak synapses. This is a power penalty. Second, setting the synaptic weights in the photonic domain means that any noise on the transmitting neuron light sources results in additional noise received by the neuron. This is an information-processing penalty.

The alternative is to set the synaptic weights in the electronic domain. The synaptic response is independent of the number of incident photons, and the synaptic weight is stored and implemented by an electronic circuit. Provided a synaptic terminal receives a photonic signal surpassing a certain threshold, a synaptic event is induced. The physical limit on the amplitude of this threshold signal is a single photon. Establishing the synaptic weight in this manner is most straightforward if each synapse is equipped with an independent photodetector. For integration with CMOS, the waveguide-integrated SiGe or defect detectors described above are good candidates. Logic circuits based on MOSFETs are the clear choice to implement synaptic, dendritic, and neuronal computations, and transistors operated in analog may play a role. Upon reaching threshold, the transistor circuits would drive a pulse through an on-chip laser, and the light thus produced would fan out to downstream connections. At those connections, as long as a number of photons greater than the threshold were received, the synaptic response would ensue, thus eliminating the effects of any noise on the photonic communication signal. The challenge here is the same at that mentioned above: it is hard to integrate light sources on silicon. If a million III-V or SiGe sources can be integrated on a 300-mm silicon optoelectronic wafer in a cost-effective manner, such an approach to optoelectronic networks will be viable.

To reach the physical limit of single-photon synaptic threshold, superconducting-nanowire single-photon detectors (SPDs) can be used. We will describe these detectors in more detail in the next section, but for the present discussion we point out that these detectors respond to single photons, and their response is nearly identical \cite{} if one or more than one photon is detected. Thus, neuronal communication using these detectors enables the lowest possible communication signal level, and sources must produce only enough photons per synaptic connection so that even with noise, each synapse receives at least one photon, with a chosen tolerable error rate. Such communication appears to saturate a physical limitation for neuronal signaling with photons of a given wavelength. Whereas transistors were used for computation in the hardware example above, if SPDs are used for detection, circuits of JJs the clear choice for computation. Because SPDs and JJs both require operation near 4.2\,K, optoelectronic hardware operating in this modality has the potential to utilize silicon light sources, potentially bringing a tremendous advantage in cost and scalability. In the next section we will describe the synaptic, dendritic, and neuronal functions of these circuits. 

\subsubsection{Where are the Light Sources?}

\paragraph{Considerations for Digital Communication}
\vspace{3em}
For digital communication it makes sense to modulate a CW laser because the photons are only discarded half the time. With neurons firing sparsely, tapping a CW light stream every time a neuron fires is wasteful. Almost all the light\textemdash which is the system's most valuable resource\textemdash is simply thrown away. If many neurons are multiplexed on the same light source, they suffer cross talk, which becomes particularly problematic when the fire synchronously. By using light sources that only generate light when the neuron fires, photons are not wasted during quiescent periods. Because each neuron has its own emitter, cross talk does not occur. 

In the case of silicon light sources based on point defects that are currently under investigation for this application, emission occurs in a sharp zero-phonon line with 0.3 nm bandwidth with x\% emitted in broader phonon assisted sidebands with 10 nm bandwidth. All emitters are identical, so design of passive components is straightforward. Whereas the presence of the phonon sideband is problematic for quantum application requiring pure quantum states, light in the phonon sideband is still useful in this application and does not represent an impediment to operation. Most importantly, the utilization of silicon light sources enables scalable fabrication unlike any other approach. These light sources are suitable for this application because the technology only requires incoherent pulses of light with 10\,ns emitter lifetime and 1\,\% efficiency. Because single-photon detectors can be utilized, the emitters do not need to be particularly bright, generating only one to 10 photons per connection to compensate for propagation loss and Poisson noise. The sources are as simple as possible, as are the detectors, and the result is a highly scalable hardware platform tailored to this type of information processing.


\paragraph{Silicon Light Sources: the Great Achilles' Heel}
So if photonic switches, modulators, filters, and detectors can all be implemented in silicon, why do all silicon microelectronic chips not have photonic components? There is one reason: a simple, inexpensive light source integrated with silicon waveguides operating at room temperature does not yet exist. Silicon has an indirect band gap, so optical emission requires a phonon for momentum conservation. This three-body process (electron, hole, phonon) is rare, so non-radiative recombination dominates. Regardless, if silicon is to be used as a passive and active waveguiding material for routing, switching, and modulation, a source emitting at a longer wavelength must achieved, just as detectors must absorb at longer wavelength, as described above. If detectors can be made to accomplish this, why is the same not true for sources? Despite efforts for decades \cite{shxu2007}, an economical, efficient, room-temperature, waveguide-integrated light source on silicon has not been discovered. To understand the source challenges, let us briefly consider three means by which researchers have attempted to create silicon light sources. More comprehensive surveys can be found in the literature \cite{li2005,shxu2007,libo2010,zhyi2015}.

%\begin{figure} 
%    \centering{\includegraphics[width=8.6cm]{silicon_absorption_emission.pdf}}
%	\captionof{figure}{\label{fig:silicon_absorption_emission}Caption.}
%\end{figure}
Like the case of detectors, two approaches to creating light sources on silicon are band gap engineering with Ge alloys and introduction of states in the gap via lattice defects. While detectors based on SiGe have shown decent performance without extensive process development, the same cannot be said of SiGe sources. Poor material quality is not as problematic if the goal is to make an absorber, whereas non-radiative recombination pathways introduced by material defects greatly limit the efficiency of SiGe as a light source and lead to high threshold current for lasing \cite{zhyi2015}. Thus, despite the process compatibility of SiGe with CMOS, SiGe lasers to date have not high enough performance with low enough cost to find a market. 

Similarly, light sources based on defects in silicon have been studied extensively for decades as the silicon microelectronics industry has matured \cite{da1989}. While defect-based detectors have demonstrated useful performance and low cost at room temperature, defect-based light sources have not. To understand why, consider a three-level model of the processes of absorption and emission, as shown in Fig.\,\ref{fig:silicon_absorption_emission}. The three levels involved are the ground state ($E_0$, electron in valence band, hole in conduction band), the first excited state ($E_1$, electron and hole bound to defect), and second excited state ($E_2$, electron in conduction band, hole in valence band). At room temperature, the two phonon mediated processes ($E_2$ $\rightarrow$ $E_1$ and $E_1$ $\rightarrow$ $E_2$) are both fast, with few-picosecond time constants (check Davies). The electric-dipole transition ($E_1$ $\rightarrow$ $E_0$) is comparatively slower, with nanosecond to millisecond transitions depending on the specific defect \cite{}. In detection, the dipole transition ($E_0$ $\rightarrow$ $E_1$) is pumped by the signal to be detected, and the excited electron-hole pair quickly transitions from $E_1$ to $E_2$, where a reverse-bias field sweeps the carriers out of the junction, resulting in detection. By contrast, in the emission process one pumps the $E_0$ $\rightarrow$ $E_2$ transition (through electrical carrier injection in a $p-n$ junction), and the excited carriers quickly transition to $E_1$, but before they can make the slow transition from $E_1$ to $E_0$, they make the fast transition back from $E_1$ to $E_2$, and eventually recombine non-radiatively through a variety of pathways without making the slow, dipole transition required to generate light. Crucially for our story, this is not the case at low temperature. The $E_2$ $\rightarrow$ $E_1$ transition involves emission of a phonon, so it remains fast, while $E_1$ $\rightarrow$ $E_2$ involves absorption of a phonon. At liquid helium temperature (4.2\,K), the relevant phonon states have low occupation, and the rate of the optical transition from $E_1$ to $E_0$ can be faster than the rate of transition back to the band edge, making silicon light sources possible based on this mechanism when operating at the same temperature required to enable superconducting circuits based on Josephson junctions. 

In addition to these two approaches to light sources on silicon, a major effort has been undertaken in the last 15 years to achieve hybrid integration of III-V light sources on silicon. Process incompatibility and lattice mismatch make it difficult to grow III-V gain media directly on silicon. Independent processing of Si and III-V substrates followed by wafer bonding is being pursued, but contemporary CMOS is very comfortable at 300-mm-wafer scale, while III-V processing has stayed at 150\,mm or below. Many such subtleties and complexities of process and materials integration have limited hybrid system performance and kept costs high. Many of the challenges are practical rather than fundamental, but nevertheless place real limits on the technologies that are achieved.

\paragraph{Hybrid Materials Integration}

\paragraph{Systems with Off-Chip Sources}
Considerable work continues in the development of light sources on silicon. At present, many efforts are proceeding to demonstrate exciting systems on chip with optical communication based on external III-V lasers fiber-coupled to silicon optoelectronic processors. Such work began commercially with the founding of Luxtera in 2001 with the goal being to utilize integrated silicon photonics for network interconnects. More recently, the effort led by Stojanovic, Popovic, and Ram has led to the development of silicon photonic systems implemented in existing CMOS processes with zero changes to the process. This effort has demonstrated basic components, such as waveguides, filters and modulators in 45-nm \cite{orma2012,shor2014,meor2014} and 32-nm technology nodes \cite{}. This ``zero-change'' approach (initially funded by DARPA \cite{}) has matured to the point where all-optical communication with 11 wavelength-division multiplexed channels was used between a processor and DRAM \cite{suwa2015,suwa2017} in the same 45-nm silicon-on-insulator process that was used to create the IBM Power 7 processor (Watson, PlayStation 3). This feat represents a significant milestone in the technological trajectory connecting global photonic networks down to optoelectronic systems on a single chip, perhaps fulfilling Soref's vision of a superchip. In this work, the III-V light sources are external to the silicon chip with fiber coupling between. Some in the field contend this will remain the most tractable solution in the long term. 

Significant commercial interest has persisted in this field since the founding of Luxtera, including major efforts by Intel \cite{}, and continuing with start-ups spinning out of the zero-change work \cite{}. All of these efforts attempt to use light as a means to communicate digital signals between electronic processors, whether it be at scale of a single chip \cite{suwa2015}, a server rack \cite{} or a data center \cite{}. As has been the case in semiconductor electronics and superconductor electronics, the device and hardware infrastructure developed for digital information processing is now being explored for neuromorphic information processing. 

\vspace{3em}
Regarding the primary motivation for optical devices, Kogelnik wrote in 1981, ``...the available speed is almost limitless, and it will be a challenge to exploit this speed.'' \cite{ko1981}

\paragraph{Silicon Light Sources Work at Low Temperature}